{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project aims to leverage toy examples to illustrate how to build linear model, deep neural networks, convolutional nueral networds as well as recurrent neural networks by [Tensorflow](www.tensorflow.org). The dataset is famous handwritten digits dataset, namely [MNIST](http://yann.lecun.com/exdb/mnist/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#Import required packages\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "#Load MNIST dataset\n",
    "import tensorflow.examples.tutorials.mnist.input_data as input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(mnist.train.images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(mnist.test.images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 55000 items of training data, and 10000 items of testing data. Let's take a look at the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x226e4ac2320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(mnist.train.images[5].reshape([28, 28]))\n",
    "print(mnist.train.labels[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEX4gKMIPBPX/EhqoeSQulYEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x226e4ac20b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(mnist.train.images[1].reshape([28, 28]))\n",
    "print(mnist.train.labels[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_tr = mnist.train.images.shape[0]# number of training samples\n",
    "n_ts = mnist.test.images.shape[0]#number of testing samples\n",
    "n_pixel = mnist.train.images.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Here's an simple linear model to do classification on MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a linear graph, define its architecture in terms of nodes and compuatations\n",
    "graph_linear = tf.Graph()\n",
    "with graph_linear.as_default():\n",
    "    #Define input placeholder and target placeholder\n",
    "    x = tf.placeholder(tf.float32, [None, 784], name='x_placeholder')\n",
    "    y_ = tf.placeholder(tf.float32, [None,10], name='y_placeholder')\n",
    "    #Define a linear model structure\n",
    "    def linearModel(x):\n",
    "        #Create weights and biases\n",
    "        W = tf.Variable(tf.zeros([784,10]), name='weights')\n",
    "        b = tf.Variable(tf.zeros([10]), name='biases')\n",
    "        #Softmax output \n",
    "        y = tf.nn.softmax(tf.matmul(x,W) + b)\n",
    "        return y\n",
    "    y = linearModel(x)\n",
    "    #Cross entropy as the loss\n",
    "    cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "    #Use gradient descent method to optmize\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy： 0.3195\n",
      "Test Accuracy： 0.9114\n",
      "Test Accuracy： 0.9193\n",
      "Test Accuracy： 0.9178\n",
      "Test Accuracy： 0.9209\n",
      "Test Accuracy： 0.9116\n",
      "Test Accuracy： 0.9121\n",
      "Test Accuracy： 0.9087\n",
      "Test Accuracy： 0.9194\n",
      "Test Accuracy： 0.9096\n",
      "Test Accuracy： 0.9225\n",
      "Test Accuracy： 0.9206\n",
      "Test Accuracy： 0.9231\n",
      "Test Accuracy： 0.9219\n",
      "Test Accuracy： 0.9223\n",
      "Test Accuracy： 0.9164\n",
      "Test Accuracy： 0.9174\n",
      "Test Accuracy： 0.9161\n",
      "Test Accuracy： 0.9257\n",
      "Test Accuracy： 0.9118\n"
     ]
    }
   ],
   "source": [
    "e_pochs = 10\n",
    "batch_size = 64\n",
    "num_steps = int(n_tr/batch_size)\n",
    "#Create a session, in order to run the graph\n",
    "with tf.Session(graph=graph_linear) as sess:\n",
    "    #Initialize all the variables created above\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    for _ in range(e_pochs):\n",
    "        for step in range(num_steps):\n",
    "            #offset = (step * batch_size) % (mnist.train.labels.shape[0] - batch_size)\n",
    "            #Feed the dataset to the placeholders     \n",
    "            batch_data, batch_labels = mnist.train.next_batch(batch_size)\n",
    "            #batch_data = mnist.train.images[offset:(offset + batch_size), :]\n",
    "            #batch_labels = mnist.train.labels[offset:(offset + batch_size), :]\n",
    "            #Pass the data to a dictionary\n",
    "            feed_dict = {x : batch_data, y_ : batch_labels}\n",
    "            #Train the model by feeding batch data\n",
    "            _, loss = sess.run([train_step, cross_entropy], feed_dict=feed_dict)\n",
    "            #Calculate acccuracy after 500 times training\n",
    "            if step%500 == 0:\n",
    "                result = sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})\n",
    "                print(\"Test Accuracy：\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Connected Model\n",
    "\n",
    "This example uses several fully connected neural network layers to train the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Created a fully connected graph\n",
    "graph_fully = tf.Graph()\n",
    "with graph_fully.as_default() as g:\n",
    "    #Define input placeholders\n",
    "    x = tf.placeholder(tf.float32, [None, 784])\n",
    "    y_ = tf.placeholder(tf.float32, [None,10])\n",
    "    num_class = y.shape[1].value\n",
    "    num_pixel = x.shape[1].value\n",
    "    #Define the architecture of neural networks\n",
    "    def multiHiddenLayer(x, layers):\n",
    "        num_layer = len(layers)\n",
    "        pre_units = num_pixel#输入节点\n",
    "        next_units = pre_units#输出节点\n",
    "        data = x\n",
    "        #Traverse each layer\n",
    "        for i in range(num_layer):            \n",
    "            with g.name_scope('hidden'+str(i)):\n",
    "                #权重\n",
    "                next_units = layers[i]\n",
    "                weights = tf.Variable(\n",
    "                    tf.truncated_normal([pre_units, next_units],\n",
    "                                        stddev=1.0 / np.sqrt(float(pre_units))),\n",
    "                    name='weights')\n",
    "                #偏差\n",
    "                biases = tf.Variable(tf.zeros([next_units]), \n",
    "                                     name='biases')\n",
    "                hidden = tf.nn.relu(tf.matmul(data, weights) + biases)\n",
    "                data = hidden\n",
    "                pre_units = next_units\n",
    "        # Linear\n",
    "        with g.name_scope('softmax_linear'):\n",
    "            weights = tf.Variable(\n",
    "                tf.truncated_normal([next_units, num_class],\n",
    "                                    stddev=1.0 / np.sqrt(float(next_units))),\n",
    "                name='weights')\n",
    "            biases = tf.Variable(tf.zeros([num_class]),\n",
    "                                 name='biases')\n",
    "            logits = tf.matmul(data, weights) + biases\n",
    "        return tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with graph_fully.as_default() as g:\n",
    "    layers = [128, 64]#neurons for each layer\n",
    "    y = multiHiddenLayer(x, layers)\n",
    "    #定义目标函数，采用交叉熵\n",
    "    cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "    #定义训练方式，梯度下降法\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.001).minimize(cross_entropy)\n",
    "    #定义模型评价指标精确度\n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy： 0.1208\n",
      "Test Accuracy： 0.9107\n",
      "Test Accuracy： 0.9245\n",
      "Test Accuracy： 0.9379\n",
      "Test Accuracy： 0.9456\n",
      "Test Accuracy： 0.9526\n",
      "Test Accuracy： 0.9559\n",
      "Test Accuracy： 0.9596\n",
      "Test Accuracy： 0.9627\n",
      "Test Accuracy： 0.9644\n",
      "Test Accuracy： 0.9671\n",
      "Test Accuracy： 0.9684\n",
      "Test Accuracy： 0.964\n",
      "Test Accuracy： 0.9685\n",
      "Test Accuracy： 0.9711\n",
      "Test Accuracy： 0.9729\n",
      "Test Accuracy： 0.9727\n",
      "Test Accuracy： 0.9749\n",
      "Test Accuracy： 0.9724\n",
      "Test Accuracy： 0.9735\n",
      "Test Accuracy： 0.9756\n",
      "Test Accuracy： 0.9726\n",
      "Test Accuracy： 0.9747\n",
      "Test Accuracy： 0.9749\n",
      "Test Accuracy： 0.9774\n",
      "Test Accuracy： 0.9761\n",
      "Test Accuracy： 0.9765\n",
      "Test Accuracy： 0.9774\n",
      "Test Accuracy： 0.9735\n",
      "Test Accuracy： 0.9744\n",
      "Test Accuracy： 0.9772\n",
      "Test Accuracy： 0.9764\n",
      "Test Accuracy： 0.9759\n",
      "Test Accuracy： 0.9772\n",
      "Test Accuracy： 0.9766\n",
      "Test Accuracy： 0.9763\n",
      "Test Accuracy： 0.9767\n",
      "Test Accuracy： 0.9764\n",
      "Test Accuracy： 0.9776\n",
      "Test Accuracy： 0.9792\n"
     ]
    }
   ],
   "source": [
    "e_pochs = 20\n",
    "batch_size = 64\n",
    "num_steps = int(n_tr/batch_size)\n",
    "#Create a session\n",
    "with tf.Session(graph=graph_fully) as sess:\n",
    "    #Initialize all the variables\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    for _ in range(e_pochs):\n",
    "        for step in range(num_steps):\n",
    "            #offset = (step * batch_size) % (mnist.train.labels.shape[0] - batch_size)\n",
    "            #Feed the dataset to the placeholders     \n",
    "            batch_data, batch_labels = mnist.train.next_batch(batch_size)\n",
    "            #Pass the data to a dictionary\n",
    "            feed_dict = {x : batch_data, y_ : batch_labels}\n",
    "            #Train the model by feeding batch data\n",
    "            _, loss = sess.run([train_step, cross_entropy], feed_dict=feed_dict)\n",
    "            #Calculate acccuracy after 500 times training\n",
    "            if step%500 == 0:\n",
    "                result = sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})\n",
    "                print(\"Test Accuracy：\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network\n",
    "\n",
    "Following example shows how to create a convolutional neural network. In addition, 'name_scope' is taken into consideration for each layer. Reference:http://wiki.jikexueyuan.com/project/tensorflow-zh/how_tos/variable_scope.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create weights\n",
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial, name='weights')\n",
    "\n",
    "#Create biases\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial, name='biases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Convolutional function\n",
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "#Max pool function\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_relu_pool(input, kernel_shape, bias_shape):\n",
    "    # Create variable named \"weights\".\n",
    "    weights = weight_variable(kernel_shape)\n",
    "    # Create variable named \"biases\".\n",
    "    biases = bias_variable(bias_shape)\n",
    "    conv = conv2d(input, weights)\n",
    "    relu = tf.nn.relu(conv + biases)\n",
    "    pool = max_pool_2x2(relu)\n",
    "    return pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnnLayer(x, keep_prob):\n",
    "    x_image = tf.reshape(x, [-1,28,28,1])\n",
    "    #First Conv\n",
    "    with tf.name_scope('hidden1'):\n",
    "        kernel_shape, bias_shape = [5, 5, 1, 32], [32] \n",
    "        h_pool1 = conv_relu_pool(x_image, kernel_shape, bias_shape)\n",
    "        \n",
    "    #Second Conv\n",
    "    with tf.name_scope('hidden2'):\n",
    "        kernel_shape, bias_shape = [5, 5, 32, 64], [64] \n",
    "        h_pool2 = conv_relu_pool(h_pool1, kernel_shape, bias_shape)\n",
    "    \n",
    "    #Fully Connected Layer\n",
    "    with tf.name_scope('fully_connected'):\n",
    "        W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "        b_fc1 = bias_variable([1024])\n",
    "        h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "        #Dropout, to prevent against overfitting      \n",
    "        h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "    #Softmax Layer\n",
    "    with tf.name_scope('softmax_layer'):\n",
    "        W_fc2 = weight_variable([1024, 10])\n",
    "        b_fc2 = bias_variable([10])\n",
    "        logits = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "    return tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph_cnn = tf.Graph()\n",
    "with graph_cnn.as_default() as g:\n",
    "    #Create input placeholders\n",
    "    x = tf.placeholder(tf.float32, [None, 784])\n",
    "    y_ = tf.placeholder(tf.float32, [None,10])\n",
    "    #Define ropout probability placholder\n",
    "    keep_prob = tf.placeholder(\"float\")   \n",
    "\n",
    "    y = cnnLayer(x, keep_prob)\n",
    "    #Define cross-entropy as loss function\n",
    "    cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "    #Adam Optimizer\n",
    "    train_step = tf.train.AdamOptimizer(0.0005).minimize(cross_entropy)\n",
    "    #Define accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 847.828\n",
      "Loss: 17.2792\n",
      "Loss: 6.03226\n",
      "Loss: 8.99566\n",
      "Loss: 0.492449\n",
      "Loss: 1.18727\n",
      "Loss: 0.539927\n",
      "Loss: 3.20441\n",
      "Loss: 4.73405\n",
      "Loss: 0.0628569\n",
      "Loss: 0.541013\n",
      "Loss: 5.22627\n",
      "Loss: 0.867604\n",
      "Loss: 2.71618\n",
      "Loss: 0.466885\n",
      "Loss: 0.0806042\n",
      "Loss: 0.0407576\n",
      "Loss: 5.98011\n",
      "Loss: 1.28675\n",
      "Loss: 0.0196769\n",
      "Testing Accuracy： 0.9925\n"
     ]
    }
   ],
   "source": [
    "e_pochs = 10\n",
    "batch_size = 64\n",
    "num_steps = int(n_tr/batch_size)\n",
    "#Create a session\n",
    "with tf.Session(graph=graph_cnn) as sess:\n",
    "    #Initialize variables\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    for _ in range(e_pochs):\n",
    "        for step in range(num_steps):\n",
    "            batch_data, batch_labels = mnist.train.next_batch(batch_size)\n",
    "            feed_dict = {x : batch_data, y_ : batch_labels, keep_prob: 0.5}\n",
    "            #Train\n",
    "            _, loss = sess.run([train_step, cross_entropy], feed_dict=feed_dict)\n",
    "            if step%500 == 0:\n",
    "                print('Loss:', loss)\n",
    "                \n",
    "    count = 0    \n",
    "    for _ in range(200):\n",
    "        batch_data, batch_labels = mnist.test.next_batch(50)\n",
    "        feed_dict = {x : batch_data, y_ : batch_labels, keep_prob: 1}\n",
    "        cp = sess.run(correct_prediction, feed_dict=feed_dict)\n",
    "        count += np.sum(cp)\n",
    "    print(\"Testing Accuracy：\", count/n_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN With Improvement\n",
    "\n",
    "In order to improve the performance of CNN, we can replace normal initialization with Xavier initialization, which is said to be more effective, in addition a declining learning rate is adopted rather than a constant one as the more close to the optimal value the smaller we expect the rate to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "initializer = tf.contrib.layers.xavier_initializer_conv2d()\n",
    "#Create weights\n",
    "def weight_variable(shape):\n",
    "  #initial = tf.truncated_normal(shape, stddev=0.01)\n",
    "  return tf.Variable(initializer(shape), name='weights')\n",
    "\n",
    "#Create biases\n",
    "def bias_variable(shape):\n",
    "  #initial = tf.constant(0.01, shape=shape)\n",
    "  return tf.Variable(initializer(shape), name='biases')\n",
    "\n",
    "#Convolutional function\n",
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "#Max pool function\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnnLayer2(x, keep_prob):\n",
    "    x_image = tf.reshape(x, [-1,28,28,1])\n",
    "    #First Conv\n",
    "    with tf.name_scope('hidden1'):\n",
    "        kernel_shape, bias_shape = [3, 3, 1, 32], [32] \n",
    "        h_pool1 = conv_relu_pool(x_image, kernel_shape, bias_shape)\n",
    "        \n",
    "    #Second Conv\n",
    "    with tf.name_scope('hidden2'):\n",
    "        kernel_shape, bias_shape = [3, 3, 32, 64], [64] \n",
    "        h_pool2 = conv_relu_pool(h_pool1, kernel_shape, bias_shape)\n",
    "    \n",
    "    #Fully Connected Layer\n",
    "    with tf.name_scope('fully_connected'):\n",
    "        W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "        b_fc1 = bias_variable([1024])\n",
    "        h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "        #Dropout, to prevent against overfitting      \n",
    "        h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "    #Softmax Layer\n",
    "    with tf.name_scope('softmax_layer'):\n",
    "        W_fc2 = weight_variable([1024, 10])\n",
    "        b_fc2 = bias_variable([10])\n",
    "        logits = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "    return tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, here we use a declining learning rate for gradient descending method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph_cnn2 = tf.Graph()\n",
    "with graph_cnn2.as_default() as g:\n",
    "    #Create input placeholders\n",
    "    x = tf.placeholder(tf.float32, [None, 784])\n",
    "    y_ = tf.placeholder(tf.float32, [None,10])\n",
    "    #Define ropout probability placholder\n",
    "    keep_prob = tf.placeholder(\"float\")   \n",
    "\n",
    "    y = cnnLayer2(x, keep_prob)\n",
    "    #Define cross-entropy as loss function\n",
    "    cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "    #Gradient Descending Optimizer\n",
    "    cur_step = tf.Variable(0, trainable=False)  # count the number of steps taken.\n",
    "    starter_learning_rate = 0.0005\n",
    "    learning_rate = tf.train.exponential_decay(starter_learning_rate, cur_step, 500, 0.96, staircase=True)\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy, global_step=cur_step)\n",
    "    #train_step = tf.train.AdamOptimizer(0.00001).minimize(cross_entropy)\n",
    "    #Define accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 147.909\n",
      "Loss: 5.70927\n",
      "Loss: 4.90338\n",
      "Loss: 4.17822\n",
      "Loss: 1.32107\n",
      "Loss: 3.26418\n",
      "Loss: 4.67591\n",
      "Loss: 6.36736\n",
      "Loss: 0.97047\n",
      "Loss: 1.15154\n",
      "Loss: 0.919087\n",
      "Loss: 1.29124\n",
      "Loss: 1.96115\n",
      "Loss: 0.6948\n",
      "Loss: 0.628241\n",
      "Loss: 0.858873\n",
      "Loss: 0.56969\n",
      "Loss: 1.07278\n",
      "Loss: 4.25916\n",
      "Loss: 0.324269\n",
      "Loss: 1.46772\n",
      "Loss: 0.54338\n",
      "Loss: 0.353937\n",
      "Loss: 0.255792\n",
      "Loss: 0.544565\n",
      "Loss: 0.198912\n",
      "Loss: 0.811475\n",
      "Loss: 0.103436\n",
      "Loss: 0.847413\n",
      "Loss: 16.1115\n",
      "Loss: 0.317075\n",
      "Loss: 0.371413\n",
      "Loss: 0.409622\n",
      "Loss: 0.32419\n",
      "Loss: 0.695689\n",
      "Loss: 0.150755\n",
      "Loss: 0.111508\n",
      "Loss: 1.06223\n",
      "Loss: 0.134771\n",
      "Loss: 0.667687\n",
      "Loss: 0.64333\n",
      "Loss: 1.4673\n",
      "Loss: 0.0713766\n",
      "Loss: 0.479754\n",
      "Loss: 0.694741\n",
      "Loss: 0.147707\n",
      "Loss: 0.175523\n",
      "Loss: 0.478875\n",
      "Loss: 0.561402\n",
      "Loss: 0.480085\n",
      "Loss: 0.596364\n",
      "Loss: 2.10838\n",
      "Loss: 0.390726\n",
      "Loss: 0.601686\n",
      "Loss: 0.722839\n",
      "Loss: 2.27915\n",
      "Loss: 0.153502\n",
      "Loss: 0.242326\n",
      "Loss: 0.461885\n",
      "Loss: 0.196564\n",
      "Testing Accuracy： 0.9983\n"
     ]
    }
   ],
   "source": [
    "e_pochs = 30\n",
    "batch_size = 64\n",
    "num_steps = int(n_tr/batch_size)\n",
    "#Create a session\n",
    "with tf.Session(graph=graph_cnn2) as sess:\n",
    "    #Initialize variables\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    #Run on GTX960 GPU\n",
    "    for _ in range(e_pochs):\n",
    "        for step in range(num_steps):\n",
    "            batch_data, batch_labels = mnist.train.next_batch(batch_size)\n",
    "            feed_dict = {x : batch_data, y_ : batch_labels, keep_prob: 0.5}\n",
    "            #Train\n",
    "            _, loss = sess.run([train_step, cross_entropy], feed_dict=feed_dict)\n",
    "            if step%500 == 0:\n",
    "                feed_dict = {x : batch_data, y_ : batch_labels, keep_prob: 1}\n",
    "                loss = sess.run(cross_entropy, feed_dict=feed_dict)\n",
    "                print('Loss:', loss)\n",
    "                \n",
    "    count = 0  \n",
    "    #Due to the limited GPU memory, we have to split the testing data into batches\n",
    "    for _ in range(200):\n",
    "        batch_data, batch_labels = mnist.train.next_batch(50)\n",
    "        feed_dict = {x : batch_data, y_ : batch_labels, keep_prob: 1}\n",
    "        cp = sess.run(correct_prediction, feed_dict=feed_dict)\n",
    "        count += np.sum(cp)\n",
    "    print(\"Testing Accuracy：\", count/n_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib import rnn\n",
    "# Network Parameters\n",
    "n_input = 28 # MNIST data input (img shape: 28*28)\n",
    "n_steps = 28 # timesteps\n",
    "n_hidden = 128 # hidden layer num of features\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "learning_rate = 0.00002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnnLayer(x):\n",
    "    '''\n",
    "    Create a Rnn layer\n",
    "    Treat each image as a series of vectors\n",
    "    the time step is n_step\n",
    "    the size of input vector is n_input\n",
    "    '''\n",
    "    # Transform X into series\n",
    "    # Reshape data to get 28 seq of 28 elements\n",
    "    x = tf.reshape(x, (-1, n_steps, n_input))\n",
    "    x = tf.transpose(x, [1, 0, 2])\n",
    "    # (n_steps*batch_size, n_input)\n",
    "    x = tf.reshape(x, [-1, n_input])\n",
    "    # (batch_size, n_input)\n",
    "    x = tf.split(x, n_steps, 0)\n",
    "    with tf.variable_scope(\"rnn_lstm\"):\n",
    "        # Define a lstm cell with tensorflow\n",
    "        lstm_cell = rnn.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "        with tf.variable_scope('rnn_layer'):\n",
    "            # Get lstm cell output\n",
    "            outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "            # Linear activation, using rnn inner loop last output\n",
    "        with tf.name_scope('fully_connected'):\n",
    "            # FUlly Connected layer\n",
    "            weights = tf.Variable(tf.random_normal([n_hidden, n_classes]), name='weights')\n",
    "            biases = tf.Variable(tf.random_normal([n_classes]), name='biases')\n",
    "    logits = tf.matmul(outputs[-1], weights) + biases\n",
    "    return tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph_rnn = tf.Graph()\n",
    "with graph_rnn.as_default():\n",
    "    # tf Graph input\n",
    "    x = tf.placeholder(tf.float32, [None, n_steps*n_input])\n",
    "    y_ = tf.placeholder(tf.float32, [None, n_classes])\n",
    "    y = rnnLayer(x)\n",
    "    # Define loss and optimizer\n",
    "    cross_entropy = -tf.reduce_mean(y_* tf.log(y))\n",
    "    \n",
    "    cur_step = tf.Variable(0, trainable=False)  # count the number of steps taken.\n",
    "    starter_learning_rate = 0.005\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(cross_entropy, tvars),\n",
    "                                      5)\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(cur_step)\n",
    "    #learning_rate = tf.train.exponential_decay(starter_learning_rate, cur_step, 1000, 0.96, staircase=True)\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    #train_op = optimizer.apply_gradients(\n",
    "        #zip(grads, tvars),\n",
    "        #global_step=cur_step)\n",
    "    optimizer = tf.train.AdamOptimizer(0.0001).minimize(cross_entropy)\n",
    "\n",
    "    # Evaluate model\n",
    "    correct_pred = tf.equal(tf.argmax(y_,1), tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.26374 Training Accuracy= 0.09375\n",
      "Loss:0.05619 Training Accuracy= 0.81250\n",
      "Loss:0.05009 Training Accuracy= 0.84375\n",
      "Loss:0.03332 Training Accuracy= 0.92188\n",
      "Loss:0.01795 Training Accuracy= 0.93750\n",
      "Loss:0.02159 Training Accuracy= 0.95312\n",
      "Loss:0.01759 Training Accuracy= 0.95312\n",
      "Loss:0.01550 Training Accuracy= 0.93750\n",
      "Loss:0.01899 Training Accuracy= 0.92188\n",
      "Loss:0.00603 Training Accuracy= 0.98438\n",
      "Loss:0.00750 Training Accuracy= 0.98438\n",
      "Loss:0.01236 Training Accuracy= 0.96875\n",
      "Loss:0.00897 Training Accuracy= 0.98438\n",
      "Loss:0.01271 Training Accuracy= 0.93750\n",
      "Loss:0.00758 Training Accuracy= 0.98438\n",
      "Loss:0.00407 Training Accuracy= 0.98438\n",
      "Loss:0.00391 Training Accuracy= 0.98438\n",
      "Loss:0.00368 Training Accuracy= 1.00000\n",
      "Loss:0.00607 Training Accuracy= 0.98438\n",
      "Loss:0.01294 Training Accuracy= 0.96875\n",
      "Loss:0.01248 Training Accuracy= 0.96875\n",
      "Loss:0.01447 Training Accuracy= 0.95312\n",
      "Loss:0.01703 Training Accuracy= 0.96875\n",
      "Loss:0.00984 Training Accuracy= 0.98438\n",
      "Loss:0.00670 Training Accuracy= 0.98438\n",
      "Loss:0.00074 Training Accuracy= 1.00000\n",
      "Loss:0.00777 Training Accuracy= 0.96875\n",
      "Loss:0.00277 Training Accuracy= 1.00000\n",
      "Loss:0.01156 Training Accuracy= 0.96875\n",
      "Loss:0.00631 Training Accuracy= 0.96875\n",
      "Loss:0.00825 Training Accuracy= 0.96875\n",
      "Loss:0.00303 Training Accuracy= 0.98438\n",
      "Loss:0.00354 Training Accuracy= 0.98438\n",
      "Loss:0.00102 Training Accuracy= 1.00000\n",
      "Loss:0.00718 Training Accuracy= 0.98438\n",
      "Loss:0.01678 Training Accuracy= 0.93750\n",
      "Loss:0.00433 Training Accuracy= 0.96875\n",
      "Loss:0.00212 Training Accuracy= 1.00000\n",
      "Loss:0.00911 Training Accuracy= 0.96875\n",
      "Loss:0.00936 Training Accuracy= 0.96875\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.9806\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "e_pochs = 20\n",
    "batch_size = 64\n",
    "num_steps = int(n_tr/batch_size)\n",
    "display_step = 600\n",
    "# Launch the graph\n",
    "with tf.Session(graph=graph_rnn) as sess:\n",
    "    # Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    # Keep training until reach max iterations\n",
    "    for _ in range(e_pochs):\n",
    "        for step in np.arange(num_steps):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        \n",
    "             # Run optimization op (backprop)\n",
    "            _, loss, acc = sess.run([optimizer, cross_entropy, accuracy], feed_dict={x: batch_x, y_: batch_y})\n",
    "            if step % display_step == 0:\n",
    "                # Calculate batch accuracy\n",
    "                print('Loss:{:.5f}'.format(loss), \"Training Accuracy= \" + \\\n",
    "                      \"{:.5f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for 128 mnist test images\n",
    "    test_data = mnist.test.images\n",
    "    test_label = mnist.test.labels\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: test_data, y_: test_label}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-layer Rnn model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we create a two-layer LSTM neural network with dropout method to prevent against overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_hidden = 128 # hidden layer num of features\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "n_input = 28\n",
    "n_steps = 28 # timesteps\n",
    "layer_num = 2 #LSTM layer number\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lstm_cell(keep_prob):\n",
    "    '''Create a basic lstm cell with dropout'''\n",
    "    cell = rnn.BasicLSTMCell(n_hidden, forget_bias=0)\n",
    "    cell = rnn.DropoutWrapper(cell, input_keep_prob=1.0, output_keep_prob=keep_prob)\n",
    "    return cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def multi_rnnLayer(x, keep_prob):\n",
    "    # Transform X into series\n",
    "    # Reshape data to get 28 seq of 28 elements\n",
    "    x = tf.reshape(x, (-1, n_steps, n_input))\n",
    "    # split input into 28 seqs with size(batch_size, input_size)\n",
    "    x = tf.unstack(x, n_steps, 1)\n",
    "    with tf.variable_scope(\"multi_rnn_lstm\"):\n",
    "        # Define a lstm cell with tensorflow\n",
    "        cell = lstm_cell\n",
    "        #Add dropout to prevent against overfitting\n",
    "        #cell = rnn.DropoutWrapper(cell=cell(), input_keep_prob=1.0, output_keep_prob=keep_prob)\n",
    "        #Create two layer LSTM network\n",
    "        #Note cell() creates different cells each time, this is very important\n",
    "        #DO NOT USE lstm_cell directly which will invoke parameter sharing problem\n",
    "        mlstm_cell = rnn.MultiRNNCell([cell(keep_prob) for _ in range(layer_num)], state_is_tuple=True)\n",
    "        with tf.variable_scope('multi_rnn_layer'):\n",
    "            # Get lstm cell output, using dynamic rnn, the input is one tensor\n",
    "            #outputs, states = tf.nn.dynamic_rnn(mlstm_cell, x, dtype=tf.float32)\n",
    "            #if we use static rnn, we need to unrollthe graphs and the tensor slices\n",
    "            #x must be a list of tensors\n",
    "            outputs, states = rnn.static_rnn(mlstm_cell, x, dtype=tf.float32)\n",
    "            #An alternative below\n",
    "            #outputs = []\n",
    "            #state = mlstm_cell.zero_state(batch_size,tf.float32)\n",
    "            #for time_step in range(n_steps):\n",
    "                #if time_step > 0: tf.get_variable_scope().reuse_variables()\n",
    "                #cell_output, state = mlstm_cell(x[:, time_step, :], state)\n",
    "                #outputs.append(cell_output)\n",
    "        # Linear activation, using rnn inner loop last output\n",
    "        with tf.name_scope('fully_connected'):\n",
    "            # FUlly Connected layer\n",
    "            weights = tf.Variable(tf.random_normal([n_hidden, n_classes]), name='weights')\n",
    "            biases = tf.Variable(tf.random_normal([n_classes]), name='biases')\n",
    "    logits = tf.matmul(outputs[-1], weights) + biases\n",
    "    return tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph_multirnn = tf.Graph()\n",
    "with graph_multirnn.as_default():\n",
    "    # tf Graph input\n",
    "    x = tf.placeholder(tf.float32, [None, n_steps*n_input])\n",
    "    y_ = tf.placeholder(tf.float32, [None, n_classes])\n",
    "    keep_prob =tf.placeholder(tf.float32)\n",
    "\n",
    "    y = multi_rnnLayer(x, keep_prob)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    cross_entropy = -tf.reduce_mean(y_* tf.log(y))\n",
    "    optimizer = tf.train.AdamOptimizer(0.0001).minimize(cross_entropy)\n",
    "\n",
    "    # Evaluate model\n",
    "    correct_pred = tf.equal(tf.argmax(y_,1), tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy= 0.14062\n",
      "Training Accuracy= 0.81250\n",
      "Training Accuracy= 0.79688\n",
      "Training Accuracy= 0.92188\n",
      "Training Accuracy= 0.95312\n",
      "Training Accuracy= 0.93750\n",
      "Training Accuracy= 0.90625\n",
      "Training Accuracy= 0.98438\n",
      "Training Accuracy= 0.92188\n",
      "Training Accuracy= 0.93750\n",
      "Training Accuracy= 0.90625\n",
      "Training Accuracy= 0.90625\n",
      "Training Accuracy= 0.92188\n",
      "Training Accuracy= 0.96875\n",
      "Training Accuracy= 0.98438\n",
      "Training Accuracy= 1.00000\n",
      "Training Accuracy= 0.93750\n",
      "Training Accuracy= 0.96875\n",
      "Training Accuracy= 1.00000\n",
      "Training Accuracy= 0.98438\n",
      "Training Accuracy= 0.98438\n",
      "Training Accuracy= 0.93750\n",
      "Training Accuracy= 0.95312\n",
      "Training Accuracy= 1.00000\n",
      "Training Accuracy= 1.00000\n",
      "Training Accuracy= 1.00000\n",
      "Training Accuracy= 1.00000\n",
      "Training Accuracy= 1.00000\n",
      "Training Accuracy= 0.98438\n",
      "Training Accuracy= 1.00000\n",
      "Training Accuracy= 0.98438\n",
      "Training Accuracy= 1.00000\n",
      "Training Accuracy= 1.00000\n",
      "Training Accuracy= 1.00000\n",
      "Training Accuracy= 0.98438\n",
      "Training Accuracy= 0.98438\n",
      "Training Accuracy= 0.98438\n",
      "Training Accuracy= 0.96875\n",
      "Training Accuracy= 1.00000\n",
      "Training Accuracy= 0.98438\n",
      "Training Accuracy= 0.98438\n",
      "Training Accuracy= 0.98438\n",
      "Training Accuracy= 0.98438\n",
      "Training Accuracy= 1.00000\n",
      "Training Accuracy= 1.00000\n",
      "Training Accuracy= 0.98438\n",
      "Training Accuracy= 0.98438\n",
      "Training Accuracy= 1.00000\n",
      "Training Accuracy= 1.00000\n",
      "Training Accuracy= 1.00000\n",
      "Training Accuracy= 0.98438\n",
      "Training Accuracy= 0.98438\n",
      "Training Accuracy= 0.98438\n",
      "Training Accuracy= 1.00000\n",
      "Training Accuracy= 1.00000\n",
      "Training Accuracy= 0.96875\n",
      "Training Accuracy= 0.98438\n",
      "Training Accuracy= 1.00000\n",
      "Training Accuracy= 0.98438\n",
      "Training Accuracy= 0.98438\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.9845\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "epochs = 30\n",
    "num_steps = int(55000/batch_size)\n",
    "display_step = 801\n",
    "# Launch the graph\n",
    "with tf.Session(graph=graph_multirnn) as sess:\n",
    "    # Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    # Keep training until reach max iterations\n",
    "    for _ in range(epochs):\n",
    "        for step in np.arange(num_steps):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop)\n",
    "            sess.run(optimizer, feed_dict={x: batch_x, y_: batch_y, keep_prob: 0.5})\n",
    "            if step % display_step == 0:\n",
    "                # Calculate batch accuracy\n",
    "                acc = sess.run(accuracy, feed_dict={x: batch_x, y_: batch_y, keep_prob:1.0})\n",
    "                print(\"Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for 128 mnist test images\n",
    "    test_data = mnist.test.images\n",
    "    test_label = mnist.test.labels\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: test_data, y_: test_label, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Bi-directional Rnn model\n",
    "\n",
    "This example shows how to apply Bidirectional RNN model to MNIST images, actually it leverages two RNN neetwork, one forward, one backward. Reference: https://github.com/aymericdamien/TensorFlow-Examples/tree/master/notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "batch_size = 128\n",
    "display_step = 500\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 28 # MNIST data input (img shape: 28*28)\n",
    "n_steps = 28 # timesteps\n",
    "n_hidden = 128 # hidden layer num of features\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def birnnLayer(x):\n",
    "\n",
    "    # Prepare data shape to match `bidirectional_rnn` function requirements\n",
    "    # Current data input shape: (batch_size, n_steps, n_input)\n",
    "    # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)\n",
    "    \n",
    "    # Permuting batch_size and n_steps\n",
    "    x = tf.transpose(x, [1, 0, 2])\n",
    "    # Reshape to (n_steps*batch_size, n_input)\n",
    "    x = tf.reshape(x, [-1, n_input])\n",
    "    # Split to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.split(x, n_steps, 0)\n",
    "        \n",
    "\n",
    "    # Define lstm cells with tensorflow\n",
    "    # Forward direction cell\n",
    "    with tf.variable_scope('BiRnnLayer'):\n",
    "        lstm_fw_cell = rnn.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "        # Backward direction cell\n",
    "        lstm_bw_cell = rnn.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "        #   Get lstm cell output\n",
    "        with tf.variable_scope('BiRnn_Structure'):\n",
    "            try:\n",
    "                #There are two outputs, one for forward cell, the other for\n",
    "                #The backward cell\n",
    "                outputs, _, _ = rnn.static_bidirectional_rnn(lstm_fw_cell, \n",
    "                                                                 lstm_bw_cell, x,\n",
    "                                                                 dtype=tf.float32)\n",
    "            except Exception: # Old TensorFlow version only returns outputs not states\n",
    "                outputs = rnn.static_bidirectional_rnn(lstm_fw_cell, \n",
    "                                                           lstm_bw_cell, x,\n",
    "                                                           dtype=tf.float32)\n",
    "        with tf.name_scope('fully_connected'):\n",
    "            # FUlly Connected layer\n",
    "            weights = tf.Variable(tf.random_normal([2*n_hidden, n_classes]), name='weights')\n",
    "            biases = tf.Variable(tf.random_normal([n_classes]), name='biases')\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.add(tf.matmul(outputs[-1], weights), biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph_biRnn = tf.Graph()\n",
    "with graph_biRnn.as_default():\n",
    "    # placeholder input\n",
    "    x = tf.placeholder(tf.float32, [None, n_steps, n_input])\n",
    "    y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "    \n",
    "    #Create a bidirectional rnn layer\n",
    "    pred = birnnLayer(x)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "    \n",
    "    #cur_step = tf.Variable(0, trainable=False)  # count the number of steps taken.\n",
    "    #starter_learning_rate = 0.005\n",
    "    #tvars = tf.trainable_variables()\n",
    "    #grads, _ = tf.clip_by_global_norm(\n",
    "        #tf.gradients(cross_entropy, tvars), 5)\n",
    "        #optimizer = tf.train.GradientDescentOptimizer(cur_step)\n",
    "    #learning_rate = tf.train.exponential_decay(starter_learning_rate, cur_step, 500, 0.90, staircase=True)\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    #train_op = optimizer.apply_gradients(\n",
    "        #zip(grads, tvars),\n",
    "        #global_step=cur_step)\n",
    "    train_op = tf.train.AdamOptimizer(0.0001).minimize(cross_entropy)\n",
    "\n",
    "    # Evaluate model\n",
    "    correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Minibatch Loss= 2.762307, Training Accuracy= 0.05469\n",
      " Minibatch Loss= 0.719144, Training Accuracy= 0.72656\n",
      " Minibatch Loss= 0.314582, Training Accuracy= 0.91406\n",
      " Minibatch Loss= 0.247267, Training Accuracy= 0.92969\n",
      " Minibatch Loss= 0.144795, Training Accuracy= 0.94531\n",
      " Minibatch Loss= 0.163616, Training Accuracy= 0.96094\n",
      " Minibatch Loss= 0.121279, Training Accuracy= 0.94531\n",
      " Minibatch Loss= 0.120281, Training Accuracy= 0.96875\n",
      " Minibatch Loss= 0.086797, Training Accuracy= 0.96875\n",
      " Minibatch Loss= 0.162958, Training Accuracy= 0.93750\n",
      " Minibatch Loss= 0.043332, Training Accuracy= 0.99219\n",
      " Minibatch Loss= 0.045126, Training Accuracy= 0.99219\n",
      " Minibatch Loss= 0.089108, Training Accuracy= 0.96875\n",
      " Minibatch Loss= 0.071346, Training Accuracy= 0.98438\n",
      " Minibatch Loss= 0.168034, Training Accuracy= 0.94531\n",
      " Minibatch Loss= 0.028106, Training Accuracy= 0.98438\n",
      " Minibatch Loss= 0.048147, Training Accuracy= 0.98438\n",
      " Minibatch Loss= 0.096012, Training Accuracy= 0.96094\n",
      " Minibatch Loss= 0.108399, Training Accuracy= 0.96875\n",
      " Minibatch Loss= 0.031771, Training Accuracy= 1.00000\n",
      " Minibatch Loss= 0.018218, Training Accuracy= 1.00000\n",
      " Minibatch Loss= 0.089941, Training Accuracy= 0.96094\n",
      " Minibatch Loss= 0.030112, Training Accuracy= 0.99219\n",
      " Minibatch Loss= 0.067277, Training Accuracy= 0.97656\n",
      " Minibatch Loss= 0.027177, Training Accuracy= 0.98438\n",
      " Minibatch Loss= 0.054646, Training Accuracy= 0.97656\n",
      " Minibatch Loss= 0.031598, Training Accuracy= 0.99219\n",
      " Minibatch Loss= 0.067667, Training Accuracy= 0.98438\n",
      " Minibatch Loss= 0.029657, Training Accuracy= 0.98438\n",
      " Minibatch Loss= 0.021682, Training Accuracy= 1.00000\n",
      " Minibatch Loss= 0.069603, Training Accuracy= 0.96875\n",
      " Minibatch Loss= 0.026557, Training Accuracy= 0.98438\n",
      " Minibatch Loss= 0.013306, Training Accuracy= 1.00000\n",
      " Minibatch Loss= 0.028145, Training Accuracy= 0.99219\n",
      " Minibatch Loss= 0.029957, Training Accuracy= 0.99219\n",
      " Minibatch Loss= 0.082082, Training Accuracy= 0.96875\n",
      " Minibatch Loss= 0.045839, Training Accuracy= 0.99219\n",
      " Minibatch Loss= 0.023492, Training Accuracy= 0.99219\n",
      " Minibatch Loss= 0.025810, Training Accuracy= 0.99219\n",
      " Minibatch Loss= 0.073001, Training Accuracy= 0.98438\n",
      " Minibatch Loss= 0.015473, Training Accuracy= 1.00000\n",
      " Minibatch Loss= 0.004992, Training Accuracy= 1.00000\n",
      " Minibatch Loss= 0.018957, Training Accuracy= 0.99219\n",
      " Minibatch Loss= 0.008486, Training Accuracy= 1.00000\n",
      " Minibatch Loss= 0.010693, Training Accuracy= 1.00000\n",
      " Minibatch Loss= 0.010478, Training Accuracy= 1.00000\n",
      " Minibatch Loss= 0.009739, Training Accuracy= 1.00000\n",
      " Minibatch Loss= 0.003472, Training Accuracy= 1.00000\n",
      " Minibatch Loss= 0.017457, Training Accuracy= 0.99219\n",
      " Minibatch Loss= 0.005251, Training Accuracy= 1.00000\n",
      " Minibatch Loss= 0.026275, Training Accuracy= 0.98438\n",
      " Minibatch Loss= 0.022792, Training Accuracy= 1.00000\n",
      " Minibatch Loss= 0.001804, Training Accuracy= 1.00000\n",
      " Minibatch Loss= 0.023057, Training Accuracy= 0.99219\n",
      " Minibatch Loss= 0.003847, Training Accuracy= 1.00000\n",
      " Minibatch Loss= 0.016004, Training Accuracy= 0.99219\n",
      " Minibatch Loss= 0.010998, Training Accuracy= 1.00000\n",
      " Minibatch Loss= 0.004749, Training Accuracy= 1.00000\n",
      " Minibatch Loss= 0.004627, Training Accuracy= 1.00000\n",
      " Minibatch Loss= 0.056017, Training Accuracy= 0.98438\n",
      " Minibatch Loss= 0.038494, Training Accuracy= 0.98438\n",
      " Minibatch Loss= 0.004677, Training Accuracy= 1.00000\n",
      " Minibatch Loss= 0.022682, Training Accuracy= 0.99219\n",
      " Minibatch Loss= 0.004541, Training Accuracy= 1.00000\n",
      " Minibatch Loss= 0.015046, Training Accuracy= 0.99219\n",
      " Minibatch Loss= 0.003273, Training Accuracy= 1.00000\n",
      " Minibatch Loss= 0.038052, Training Accuracy= 0.98438\n",
      " Minibatch Loss= 0.003618, Training Accuracy= 1.00000\n",
      " Minibatch Loss= 0.002781, Training Accuracy= 1.00000\n",
      " Minibatch Loss= 0.003020, Training Accuracy= 1.00000\n",
      " Minibatch Loss= 0.012857, Training Accuracy= 0.99219\n",
      " Minibatch Loss= 0.004025, Training Accuracy= 1.00000\n",
      " Minibatch Loss= 0.001989, Training Accuracy= 1.00000\n",
      " Minibatch Loss= 0.011197, Training Accuracy= 0.99219\n",
      " Minibatch Loss= 0.002708, Training Accuracy= 1.00000\n",
      " Minibatch Loss= 0.011714, Training Accuracy= 1.00000\n",
      " Minibatch Loss= 0.003115, Training Accuracy= 1.00000\n",
      " Minibatch Loss= 0.002617, Training Accuracy= 1.00000\n",
      " Minibatch Loss= 0.004122, Training Accuracy= 1.00000\n",
      " Minibatch Loss= 0.002620, Training Accuracy= 1.00000\n",
      " Minibatch Loss= 0.001599, Training Accuracy= 1.00000\n",
      " Minibatch Loss= 0.004119, Training Accuracy= 1.00000\n",
      " Minibatch Loss= 0.003848, Training Accuracy= 1.00000\n",
      " Minibatch Loss= 0.005802, Training Accuracy= 1.00000\n",
      " Minibatch Loss= 0.000290, Training Accuracy= 1.00000\n",
      " Minibatch Loss= 0.004871, Training Accuracy= 1.00000\n",
      " Minibatch Loss= 0.006521, Training Accuracy= 1.00000\n",
      " Minibatch Loss= 0.001565, Training Accuracy= 1.00000\n",
      " Minibatch Loss= 0.001558, Training Accuracy= 1.00000\n",
      " Minibatch Loss= 0.000963, Training Accuracy= 1.00000\n",
      " Minibatch Loss= 0.001437, Training Accuracy= 1.00000\n",
      " Minibatch Loss= 0.000768, Training Accuracy= 1.00000\n",
      " Minibatch Loss= 0.002629, Training Accuracy= 1.00000\n",
      " Minibatch Loss= 0.000272, Training Accuracy= 1.00000\n",
      " Minibatch Loss= 0.000525, Training Accuracy= 1.00000\n",
      " Minibatch Loss= 0.007589, Training Accuracy= 0.99219\n",
      " Minibatch Loss= 0.000848, Training Accuracy= 1.00000\n",
      " Minibatch Loss= 0.004125, Training Accuracy= 1.00000\n",
      " Minibatch Loss= 0.001156, Training Accuracy= 1.00000\n",
      " Minibatch Loss= 0.000990, Training Accuracy= 1.00000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.9851\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "epochs = 100\n",
    "num_steps = int(55000/batch_size)\n",
    "display_step = 801\n",
    "# Create a session to run the model\n",
    "with tf.Session(graph=graph_biRnn) as sess:\n",
    "    # Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    for _ in range(epochs):\n",
    "        # Keep training until reach max iterations\n",
    "        for step in np.arange(num_steps):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            # Reshape data to get 28 seq of 28 elements\n",
    "            batch_x = batch_x.reshape((batch_size, n_steps, n_input))\n",
    "            # Run optimization op (backprop)\n",
    "            sess.run(train_op, feed_dict={x: batch_x, y: batch_y})\n",
    "            if step % display_step == 0:\n",
    "                # Calculate batch accuracy\n",
    "                acc = sess.run(accuracy, feed_dict={x: batch_x, y: batch_y})\n",
    "                # Calculate batch loss\n",
    "                loss = sess.run(cross_entropy, feed_dict={x: batch_x, y: batch_y})\n",
    "                print(\" Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for 128 mnist test images\n",
    "    test_data = mnist.test.images.reshape((-1, n_steps, n_input))\n",
    "    test_label = mnist.test.labels\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: test_data, y: test_label}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Adversarial Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows how to use generative adversarial model to create fake MNIST images and try to distinguish them. Referrence: https://github.com/wiseodd/generative-models/tree/master/GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import required packages\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the architecture for generator and discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph_gan = tf.Graph()\n",
    "with graph_gan.as_default():\n",
    "    #Define xavier initializing method, tries to keep variance concentrated\n",
    "    #http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2010_GlorotB10.pdf\n",
    "    initializer = tf.contrib.layers.xavier_initializer()\n",
    "    #Create Discriminator\n",
    "    X = tf.placeholder(tf.float32, shape=[None, 784])  \n",
    "    D_W1 = tf.Variable(initializer([784, 128]))\n",
    "    D_b1 = tf.Variable(tf.zeros(shape=[128]))\n",
    "    D_W2 = tf.Variable(initializer([128, 1]))\n",
    "    D_b2 = tf.Variable(tf.zeros(shape=[1]))\n",
    "    theta_D = [D_W1, D_W2, D_b1, D_b2]\n",
    "    #Create Discriminator\n",
    "    #Judge whether an image is real\n",
    "    #Discriminate the input digit\n",
    "    #Return a probability\n",
    "    def discriminator(x):\n",
    "        D_h1 = tf.nn.relu(tf.matmul(x, D_W1) + D_b1)\n",
    "        D_logit = tf.matmul(D_h1, D_W2) + D_b2\n",
    "        D_prob = tf.nn.sigmoid(D_logit)\n",
    "        return D_prob, D_logit\n",
    "    \n",
    "    #Parameters for generator\n",
    "    #Z is a noise to generate an image\n",
    "    Z = tf.placeholder(tf.float32, shape=[None, 100])\n",
    "    G_W1 = tf.Variable(initializer([100, 128]))\n",
    "    G_b1 = tf.Variable(tf.zeros(shape=[128]))\n",
    "    G_W2 = tf.Variable(initializer([128, 784]))\n",
    "    G_b2 = tf.Variable(tf.zeros(shape=[784]))\n",
    "    theta_G = [G_W1, G_W2, G_b1, G_b2]\n",
    "    \n",
    "    #Generate a digit from random noise\n",
    "    #Return a fake digit \n",
    "    def generator(z):\n",
    "        G_h1 = tf.nn.relu(tf.matmul(z, G_W1) + G_b1)\n",
    "        G_log_prob = tf.matmul(G_h1, G_W2) + G_b2\n",
    "        G_prob = tf.nn.sigmoid(G_log_prob)\n",
    "        return G_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_Z(m, n):\n",
    "    return np.random.uniform(-1., 1., size=[m, n])\n",
    "\n",
    "#Plot the samples\n",
    "def plot(samples):\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with graph_gan.as_default():\n",
    "    G_sample = generator(Z)\n",
    "    #For real digit image\n",
    "    D_real, D_logit_real = discriminator(X)\n",
    "    #For generated digit image\n",
    "    D_fake, D_logit_fake = discriminator(G_sample)\n",
    "    \n",
    "    #There are two steps\n",
    "    #For the discriminator\n",
    "    #The probability of real should be larger \n",
    "    #whereas the probability for the generated digit image should be lower\n",
    "    D_loss = -tf.reduce_mean(tf.log(D_real) + tf.log(1. - D_fake))\n",
    "    #For the generator, it intends to make the fake images more real\n",
    "    G_loss = -tf.reduce_mean(tf.log(D_fake))\n",
    "    \n",
    "    #Two optimizer for both descriminator and generator\n",
    "    D_optimizer = tf.train.AdamOptimizer().minimize(D_loss, var_list=theta_D)\n",
    "    G_optimizer = tf.train.AdamOptimizer().minimize(G_loss, var_list=theta_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0\n",
      "D loss: 1.43\n",
      "G_loss: 2.573\n",
      "\n",
      "Iter: 1000\n",
      "D loss: 0.004664\n",
      "G_loss: 7.032\n",
      "\n",
      "Iter: 2000\n",
      "D loss: 0.02508\n",
      "G_loss: 5.806\n",
      "\n",
      "Iter: 3000\n",
      "D loss: 0.03886\n",
      "G_loss: 5.96\n",
      "\n",
      "Iter: 4000\n",
      "D loss: 0.08969\n",
      "G_loss: 5.131\n",
      "\n",
      "Iter: 5000\n",
      "D loss: 0.2752\n",
      "G_loss: 5.613\n",
      "\n",
      "Iter: 6000\n",
      "D loss: 0.3167\n",
      "G_loss: 4.496\n",
      "\n",
      "Iter: 7000\n",
      "D loss: 0.4579\n",
      "G_loss: 3.85\n",
      "\n",
      "Iter: 8000\n",
      "D loss: 0.4204\n",
      "G_loss: 3.968\n",
      "\n",
      "Iter: 9000\n",
      "D loss: 0.5905\n",
      "G_loss: 2.849\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "Z_dim = 100\n",
    "\n",
    "with tf.Session(graph=graph_gan) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #Set the output path for faked digit images\n",
    "    if not os.path.exists('out/'):\n",
    "        os.makedirs('out/')\n",
    "    i = 0\n",
    "    for it in np.arange(10000):\n",
    "        if it % 1000 == 0:\n",
    "            #Generate faked digit images and save them\n",
    "            samples = sess.run(G_sample, feed_dict={Z: sample_Z(16, Z_dim)})\n",
    "            fig = plot(samples)\n",
    "            plt.savefig('out/{}.png'.format(str(i).zfill(3)), bbox_inches='tight')\n",
    "            i += 1\n",
    "            plt.close(fig)\n",
    "\n",
    "        X_mb, _ = mnist.train.next_batch(batch_size)\n",
    "\n",
    "        _, D_loss_curr = sess.run([D_optimizer, D_loss], feed_dict={X: X_mb, Z: sample_Z(batch_size, Z_dim)})\n",
    "        _, G_loss_curr = sess.run([G_optimizer, G_loss], feed_dict={Z: sample_Z(batch_size, Z_dim)})\n",
    "\n",
    "        if it % 1000 == 0:\n",
    "            print('Iter: {}'.format(it))\n",
    "            print('D loss: {:.4}'. format(D_loss_curr))\n",
    "            print('G_loss: {:.4}'.format(G_loss_curr))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPMAAADuCAYAAADsvjF6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f294b8cfd0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
