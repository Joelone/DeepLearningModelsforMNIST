{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#Import required packages\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "#Load MNIST dataset\n",
    "import tensorflow.examples.tutorials.mnist.input_data as input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20e5d854f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(mnist.train.images[5].reshape([28, 28]))\n",
    "print(mnist.train.labels[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_tr = mnist.train.images.shape[0]# number of training samples\n",
    "n_ts = mnist.test.images.shape[0]#number of testing samples\n",
    "n_pixel = mnist.train.images.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Feedforward Example \n",
    "Please refer to：https://r2rt.com/implementing-batch-normalization-in-tensorflow.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Nomalization For CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create weights\n",
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.01)\n",
    "  return tf.Variable(initial, name='weights')\n",
    "\n",
    "#Create biases\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.01, shape=shape)\n",
    "  return tf.Variable(initial, name='biases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Convolutional function\n",
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "#Max pool function\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decay = 0.9\n",
    "epsilon = 0.001\n",
    "def conv_relu_pool(x, kernel_shape, bias_shape, is_training):\n",
    "    # Create variable named \"weights\".\n",
    "    weights = weight_variable(kernel_shape)\n",
    "    # Create variable named \"biases\".\n",
    "    biases = bias_variable(bias_shape)\n",
    "    \n",
    "    # Create two new parameters, gamma and beta (shift)\n",
    "    #The bias for batch normalization\n",
    "    gamma = tf.Variable(tf.ones(bias_shape))\n",
    "    #\n",
    "    beta = tf.Variable(tf.zeros(bias_shape))\n",
    "    \n",
    "    conv = conv2d(x, weights)    \n",
    "    z = conv + biases\n",
    "    \n",
    "    moving_mean = tf.Variable(tf.zeros([z.get_shape()[-1]]), trainable=False)\n",
    "    moving_var = tf.Variable(tf.ones([z.get_shape()[-1]]), trainable=False)\n",
    "    axis = list(range(len(z.get_shape()) - 1))\n",
    "    #If training\n",
    "    if is_training:\n",
    "        batch_mean, batch_var = tf.nn.moments(z, axis)\n",
    "        train_mean = tf.assign(moving_mean,\n",
    "                               moving_mean * decay + batch_mean * (1 - decay))\n",
    "        train_var = tf.assign(moving_var,\n",
    "                              moving_var * decay + batch_var * (1 - decay))\n",
    "        with tf.control_dependencies([train_mean, train_var]):\n",
    "            bn = tf.nn.batch_normalization(z,\n",
    "                                           batch_mean, batch_var, beta, gamma, epsilon)    \n",
    "    else:\n",
    "        bn = tf.nn.batch_normalization(z, moving_mean, moving_var, beta, gamma, epsilon)\n",
    "    #Activation    \n",
    "    relu = tf.nn.relu(bn)\n",
    "    pool = max_pool_2x2(relu)\n",
    "    return pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnnLayer(x, keep_prob, is_training):\n",
    "    x_image = tf.reshape(x, [-1,28,28,1])\n",
    "    #First Conv\n",
    "    with tf.name_scope('hidden1'):\n",
    "        kernel_shape, bias_shape = [5, 5, 1, 32], [32] \n",
    "        h_pool1 = conv_relu_pool(x_image, kernel_shape, bias_shape, is_training)\n",
    "        \n",
    "    #Second Conv\n",
    "    with tf.name_scope('hidden2'):\n",
    "        kernel_shape, bias_shape = [5, 5, 32, 64], [64] \n",
    "        h_pool2 = conv_relu_pool(h_pool1, kernel_shape, bias_shape, is_training)\n",
    "    \n",
    "    #Fully Connected Layer\n",
    "    with tf.name_scope('fully_connected'):\n",
    "        W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "        b_fc1 = bias_variable([1024])\n",
    "        h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "        #Dropout, to prevent against overfitting      \n",
    "        h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "    #Softmax Layer\n",
    "    with tf.variable_scope('softmax_layer'):\n",
    "        W_fc2 = weight_variable([1024, 10])\n",
    "        b_fc2 = bias_variable([10])\n",
    "        logits = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "    return tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph_cnn = tf.Graph()\n",
    "with graph_cnn.as_default() as g:\n",
    "    #Create input placeholders\n",
    "    x = tf.placeholder(tf.float32, [None, 784])\n",
    "    y_ = tf.placeholder(tf.float32, [None,10])\n",
    "    #Define ropout probability placholder\n",
    "    keep_prob = tf.placeholder(\"float\")   \n",
    "\n",
    "    y = cnnLayer(x, keep_prob, True)\n",
    "    #Define cross-entropy as loss function\n",
    "    cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "    #Adam Optimizer\n",
    "    train_step = tf.train.AdamOptimizer(0.0005).minimize(cross_entropy)\n",
    "    #Define accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    saver = tf.train.Saver()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 124.399\n",
      "Loss: 0.370755\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "\n",
    "batch_size = 64\n",
    "num_steps = int(n_tr/batch_size)\n",
    "#Create a session\n",
    "with tf.Session(graph=graph_cnn) as sess:\n",
    "    #Initialize variables\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    for _ in range(epochs):\n",
    "        for step in range(num_steps):\n",
    "            batch_data, batch_labels = mnist.train.next_batch(batch_size)\n",
    "            feed_dict = {x : batch_data, y_ : batch_labels, keep_prob: 0.5}\n",
    "            #Train\n",
    "            _, loss = sess.run([train_step, cross_entropy], feed_dict=feed_dict)\n",
    "            if step%500 == 0:\n",
    "                feed_dict = {x : batch_data, y_ : batch_labels, keep_prob: 1}\n",
    "                loss = sess.run(cross_entropy, feed_dict=feed_dict)\n",
    "                print('Loss:', loss)\n",
    "     \n",
    "    saved_model = saver.save(sess, 'temp/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with graph_cnn.as_default() as g:\n",
    "    y = cnnLayer(x, keep_prob, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from temp\\model.ckpt\n",
      "Testing Accuracy： 0.981\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph_cnn) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    ckpt = tf.train.get_checkpoint_state('temp')\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    count = 0    \n",
    "    for _ in range(200):\n",
    "        batch_data, batch_labels = mnist.train.next_batch(50)\n",
    "        feed_dict = {x : batch_data, y_ : batch_labels, keep_prob: 1}\n",
    "        cp = sess.run(correct_prediction, feed_dict=feed_dict)\n",
    "        count += np.sum(cp)\n",
    "    print(\"Testing Accuracy：\", count/n_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Below code was referred to http://danijar.com/structuring-your-tensorflow-models/\n",
    "import functools\n",
    "def doublewrap(function):\n",
    "    \"\"\"\n",
    "    A decorator decorator, allowing to use the decorator to be used without\n",
    "    parentheses if not arguments are provided. All arguments must be optional.\n",
    "    \"\"\"\n",
    "    @functools.wraps(function)\n",
    "    def decorator(*args, **kwargs):\n",
    "        if len(args) == 1 and len(kwargs) == 0 and callable(args[0]):\n",
    "            return function(args[0])\n",
    "        else:\n",
    "            return lambda wrapee: function(wrapee, *args, **kwargs)\n",
    "    return decorator\n",
    "\n",
    "\n",
    "@doublewrap\n",
    "def define_scope(function, scope=None, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    A decorator for functions that define TensorFlow operations. The wrapped\n",
    "    function will only be executed once. Subsequent calls to it will directly\n",
    "    return the result so that operations are added to the graph only once.\n",
    "\n",
    "    The operations added by the function live within a tf.variable_scope(). If\n",
    "    this decorator is used with arguments, they will be forwarded to the\n",
    "    variable scope. The scope name defaults to the name of the wrapped\n",
    "    function.\n",
    "    \"\"\"\n",
    "    attribute = '_cache_' + function.__name__#Get function name\n",
    "    name = scope or function.__name__\n",
    "    @property\n",
    "    @functools.wraps(function)#Keep the original function\n",
    "    def decorator(self):\n",
    "        if not hasattr(self, attribute):#If the attribute not exist\n",
    "            with tf.variable_scope(name, *args, **kwargs):#Add scope name\n",
    "                setattr(self, attribute, function(self))\n",
    "        return getattr(self, attribute)#otherwise return the attribute\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#定义一个MNIST分类的基类\n",
    "class MnistModel:\n",
    "    '''Define a basic model for MNIST image classification, the model\n",
    "    Provides graph structure of tensorflow'''\n",
    "    \n",
    "    def __init__(self, input_holder, target_holder, is_training, keep_prob):\n",
    "        self.input_holder = input_holder\n",
    "        self.target_holder = target_holder\n",
    "        self.is_training = is_training\n",
    "        self.num_pixel = 784\n",
    "        self.num_class = 10\n",
    "        self.keep_prob = keep_prob\n",
    "        self.prediction\n",
    "        self.optimize\n",
    "        self.accuracy\n",
    "        print('Initializing Mnist Model!') \n",
    "    \n",
    "    @define_scope(initializer=tf.contrib.slim.xavier_initializer())\n",
    "    def prediction(self):\n",
    "        return None\n",
    "    \n",
    "    @define_scope\n",
    "    def optimize(self):\n",
    "        return None\n",
    "    \n",
    "    @define_scope\n",
    "    def accuracy(self):\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:https://github.com/ry/tensorflow-resnet/blob/master/resnet.py\n",
    "https://gist.github.com/tomokishii/0ce3bdac1588b5cca9fa5fbdf6e1c412\n",
    "https://r2rt.com/implementing-batch-normalization-in-tensorflow.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.training import moving_averages\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "class CnnMnistModel(MnistModel):\n",
    "    #重构某些函数\n",
    "    \n",
    "    @define_scope(initializer=tf.contrib.slim.xavier_initializer())\n",
    "    def prediction(self):\n",
    "        #定义权重和偏置参数变量\n",
    "        logits = self.cnnLayer\n",
    "        return tf.nn.softmax(logits)\n",
    "\n",
    "    @define_scope\n",
    "    def optimize(self):\n",
    "        cross_entropy = -tf.reduce_sum(self.target_holder*\n",
    "                                       tf.log(self.prediction))\n",
    "        optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "        return optimizer.minimize(cross_entropy)\n",
    "\n",
    "    @define_scope\n",
    "    def accuracy(self):\n",
    "        correct_prediction = tf.equal(tf.argmax(self.target_holder,1), \n",
    "                                      tf.argmax(self.prediction,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        return accuracy\n",
    "    \n",
    "    @define_scope\n",
    "    def cnnLayer(self):\n",
    "        x_image = tf.reshape(self.input_holder, [-1,28,28,1])\n",
    "        #First Conv\n",
    "        with tf.name_scope('hidden1'):\n",
    "            kernel_shape, bias_shape = [5, 5, 1, 32], [32] \n",
    "            h_pool1 = self.conv_relu_bn_pool(x_image, kernel_shape, bias_shape)\n",
    "            #Second Conv\n",
    "        with tf.name_scope('hidden2'):\n",
    "            kernel_shape, bias_shape = [5, 5, 32, 64], [64] \n",
    "            h_pool2 = self.conv_relu_bn_pool(h_pool1, kernel_shape, bias_shape)\n",
    "    \n",
    "        #Fully Connected Layer\n",
    "        with tf.name_scope('fully_connected'):\n",
    "            W_fc1 = self.weight_variable([7 * 7 * 64, 1024])\n",
    "            b_fc1 = self.bias_variable([1024])\n",
    "            h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "            h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "        #Dropout, to prevent against overfitting      \n",
    "        h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "        #Softmax Layer\n",
    "        with tf.name_scope('softmax_layer'):\n",
    "            W_fc2 = weight_variable([1024, 10])\n",
    "            b_fc2 = bias_variable([10])\n",
    "            logits = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "            \n",
    "        return logits\n",
    "    \n",
    "    #卷积函数\n",
    "    def conv2d(self, x, W):\n",
    "        return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    \n",
    "    #池化函数\n",
    "    def max_pool_2x2(self, x):\n",
    "        return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                              strides=[1, 2, 2, 1], padding='SAME')\n",
    "    \n",
    "    #Create weights\n",
    "    def weight_variable(self, shape):\n",
    "        initial = tf.truncated_normal(shape, stddev=0.01)\n",
    "        return tf.Variable(initial, name='weights')\n",
    "\n",
    "    #Create biases\n",
    "    def bias_variable(self, shape):\n",
    "        initial = tf.constant(0.01, shape=shape)\n",
    "        return tf.Variable(initial, name='biases')\n",
    "    \n",
    "\n",
    "    def _get_variable(self, name,\n",
    "                  shape,\n",
    "                  initializer,\n",
    "                  weight_decay=0.0,\n",
    "                  trainable=True):\n",
    "        \"A little wrapper around tf.get_variable to do weight decay and add to\"\n",
    "        \"resnet collection\"\n",
    "\n",
    "            \n",
    "        return tf.get_variable(name,\n",
    "                           shape=shape,\n",
    "                           initializer=initializer,\n",
    "                           dtype=tf.float32,\n",
    "                           trainable=trainable)\n",
    "\n",
    "    def conv_relu_bn_pool(self, x, kernel_shape, bias_shape):\n",
    "        # Create variable named \"weights\".\n",
    "        weights = self.weight_variable(kernel_shape)\n",
    "        # Create variable named \"biases\".\n",
    "        biases = self.bias_variable(bias_shape)\n",
    "        conv = self.conv2d(x, weights)\n",
    "        bn = self.batch_normalization(conv)\n",
    "        relu = tf.nn.relu(bn + biases)\n",
    "        pool = self.max_pool_2x2(relu)\n",
    "        return pool\n",
    "    \n",
    "\n",
    "\n",
    "    def batch_normalization(self, x):\n",
    "        x_shape = x.get_shape()\n",
    "        params_shape = x_shape[-1:]\n",
    "        axis = list(range(len(x_shape) - 1))\n",
    "        #The bias for batch normalization\n",
    "        beta = self._get_variable('beta',\n",
    "                             params_shape,\n",
    "                             initializer=tf.zeros_initializer)\n",
    "        #The scale of batch normalization\n",
    "        gamma = self._get_variable('gamma',\n",
    "                              params_shape,\n",
    "                              initializer=tf.ones_initializer)\n",
    "        #Record moving average for testing\n",
    "        moving_mean = self._get_variable('moving_mean',\n",
    "                                    params_shape,\n",
    "                                    initializer=tf.zeros_initializer,\n",
    "                                    trainable=False)\n",
    "        #Record moving average for testing\n",
    "        moving_variance = self._get_variable('moving_variance',\n",
    "                                    params_shape,\n",
    "                                    initializer=tf.ones_initializer,\n",
    "                                    trainable=False)  \n",
    "        # These ops will only be preformed when training.\n",
    "        mean, variance = tf.nn.moments(x, axis)\n",
    "        #Update moving averages\n",
    "        update_moving_mean = moving_averages.assign_moving_average(moving_mean,\n",
    "                                                               mean, 0.9997)\n",
    "        update_moving_variance = moving_averages.assign_moving_average(moving_variance, \n",
    "                                                                       variance, 0.9997)\n",
    "        tf.add_to_collection(\"UPDATE_OPS_COLLECTION\", update_moving_mean)\n",
    "        tf.add_to_collection(\"UPDATE_OPS_COLLECTION\", update_moving_variance)        \n",
    "        if self.is_training:\n",
    "            mean, variance = mean, variance\n",
    "        else:\n",
    "            mean, variance = moving_mean, moving_variance \n",
    "        x = tf.nn.batch_normalization(x, mean, variance, beta, gamma, 0.001)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable prediction/cnnLayer/beta already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-cb215b290520>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[1;31m#输入数据对应标签\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtarget_holder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_class\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mcnn_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCnnMnistModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_holder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_holder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-49-44f50d5db830>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_holder, target_holder, is_training, keep_prob)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeep_prob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-d65e7ab6eacc>\u001b[0m in \u001b[0;36mdecorator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattribute\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;31m#If the attribute not exist\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;31m#Add scope name\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                 \u001b[0msetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattribute\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattribute\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#otherwise return the attribute\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-66-d67bf6e41e6b>\u001b[0m in \u001b[0;36mprediction\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[1;31m#定义权重和偏置参数变量\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcnnLayer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-d65e7ab6eacc>\u001b[0m in \u001b[0;36mdecorator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattribute\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;31m#If the attribute not exist\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;31m#Add scope name\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                 \u001b[0msetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattribute\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattribute\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#otherwise return the attribute\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-66-d67bf6e41e6b>\u001b[0m in \u001b[0;36mcnnLayer\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'hidden1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mkernel_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mh_pool1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv_relu_bn_pool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_image\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[1;31m#Second Conv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'hidden2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-66-d67bf6e41e6b>\u001b[0m in \u001b[0;36mconv_relu_bn_pool\u001b[0;34m(self, x, kernel_shape, bias_shape)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mbiases\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mconv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mbn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_normalization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0mrelu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbn\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbiases\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mpool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_pool_2x2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-66-d67bf6e41e6b>\u001b[0m in \u001b[0;36mbatch_normalization\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    105\u001b[0m         beta = self._get_variable('beta',\n\u001b[1;32m    106\u001b[0m                              \u001b[0mparams_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                              initializer=tf.zeros_initializer)\n\u001b[0m\u001b[1;32m    108\u001b[0m         gamma = self._get_variable('gamma',\n\u001b[1;32m    109\u001b[0m                               \u001b[0mparams_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-66-d67bf6e41e6b>\u001b[0m in \u001b[0;36m_get_variable\u001b[0;34m(self, name, shape, initializer, weight_decay, trainable)\u001b[0m\n\u001b[1;32m     85\u001b[0m                            \u001b[0minitializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                            \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                            trainable=trainable)\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mconv_relu_bn_pool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[1;32m   1063\u001b[0m       \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1065\u001b[0;31m       use_resource=use_resource, custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m   1066\u001b[0m get_variable_or_local_docstring = (\n\u001b[1;32m   1067\u001b[0m     \"\"\"%s\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[1;32m    960\u001b[0m           \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 962\u001b[0;31m           use_resource=use_resource, custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m    963\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[1;32m    365\u001b[0m           \u001b[0mreuse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m           validate_shape=validate_shape, use_resource=use_resource)\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource)\u001b[0m\n\u001b[1;32m    350\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m           use_resource=use_resource)\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource)\u001b[0m\n\u001b[1;32m    662\u001b[0m                          \u001b[1;34m\" Did you mean to set reuse=True in VarScope? \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 664\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    665\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable prediction/cnnLayer/beta already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "batch_size = 64\n",
    "num_pixel = 784\n",
    "num_class = 10\n",
    "#定义feed数据\n",
    "#定义输入数据\n",
    "input_holder = tf.placeholder(tf.float32, [None, num_pixel])\n",
    "#输入数据对应标签\n",
    "target_holder = tf.placeholder(tf.float32, [None,num_class])\n",
    "cnn_model = CnnMnistModel(input_holder, target_holder, True, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
