{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project aims to leverage toy examples to illustrate how to build linear model, deep neural networks, convolutional nueral networds as well as recurrent neural networks by [Tensorflow](www.tensorflow.org). The dataset is famous handwritten digits dataset, namely [MNIST](http://yann.lecun.com/exdb/mnist/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#Import required packages\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "#Load MNIST dataset\n",
    "import tensorflow.examples.tutorials.mnist.input_data as input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Here's an simple linear model to do classification on MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a linear graph, define its architecture in terms of nodes and compuatations\n",
    "graph_linear = tf.Graph()\n",
    "with graph_linear.as_default():\n",
    "    #Define input placeholder and target placeholder\n",
    "    x = tf.placeholder(tf.float32, [None, 784], name='x_placeholder')\n",
    "    y_ = tf.placeholder(tf.float32, [None,10], name='y_placeholder')\n",
    "    #Define a linear model structure\n",
    "    def linearModel(x):\n",
    "        #Create weights and biases\n",
    "        W = tf.Variable(tf.zeros([784,10]), name='weights')\n",
    "        b = tf.Variable(tf.zeros([10]), name='biases')\n",
    "        #Softmax output \n",
    "        y = tf.nn.softmax(tf.matmul(x,W) + b)\n",
    "        return y\n",
    "    y = linearModel(x)\n",
    "    #Cross entropy as the loss\n",
    "    cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "    #Use gradient descent method to optmize\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy： 0.3045\n",
      "Test Accuracy： 0.9028\n",
      "Test Accuracy： 0.8992\n",
      "Test Accuracy： 0.9103\n",
      "Test Accuracy： 0.9211\n",
      "Test Accuracy： 0.9175\n",
      "Test Accuracy： 0.9187\n",
      "Test Accuracy： 0.9185\n",
      "Test Accuracy： 0.9005\n",
      "Test Accuracy： 0.9195\n",
      "Test Accuracy： 0.919\n"
     ]
    }
   ],
   "source": [
    "num_steps = 5001\n",
    "batch_size = 64\n",
    "#Create a session, in order to run the graph\n",
    "with tf.Session(graph=graph_linear) as sess:\n",
    "    #Initialize all the variables created above\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (mnist.train.labels.shape[0] - batch_size)\n",
    "        #Feed the dataset to the placeholders      \n",
    "        batch_data = mnist.train.images[offset:(offset + batch_size), :]\n",
    "        batch_labels = mnist.train.labels[offset:(offset + batch_size), :]\n",
    "        #Pass the data to a dictionary\n",
    "        feed_dict = {x : batch_data, y_ : batch_labels}\n",
    "        #Train the model by feeding batch data\n",
    "        _, loss = sess.run([train_step, cross_entropy], feed_dict=feed_dict)\n",
    "        #Calculate acccuracy after 500 times training\n",
    "        if step%500 == 0:\n",
    "            result = sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})\n",
    "            print(\"Test Accuracy：\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Connected Model\n",
    "\n",
    "This example uses several fully connected neural network layers to train the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Created a fully connected graph\n",
    "graph_fully = tf.Graph()\n",
    "with graph_fully.as_default() as g:\n",
    "    #Define input placeholders\n",
    "    x = tf.placeholder(tf.float32, [None, 784])\n",
    "    y_ = tf.placeholder(tf.float32, [None,10])\n",
    "    num_class = y.shape[1].value\n",
    "    num_pixel = x.shape[1].value\n",
    "    #Define the architecture of neural networks\n",
    "    def multiHiddenLayer(x, layers):\n",
    "        num_layer = len(layers)\n",
    "        pre_units = num_pixel#输入节点\n",
    "        next_units = pre_units#输出节点\n",
    "        data = x\n",
    "        #Traverse each layer\n",
    "        for i in range(num_layer):            \n",
    "            with g.name_scope('hidden'+str(i)):\n",
    "                #权重\n",
    "                next_units = layers[i]\n",
    "                weights = tf.Variable(\n",
    "                    tf.truncated_normal([pre_units, next_units],\n",
    "                                        stddev=1.0 / np.sqrt(float(pre_units))),\n",
    "                    name='weights')\n",
    "                #偏差\n",
    "                biases = tf.Variable(tf.zeros([next_units]), \n",
    "                                     name='biases')\n",
    "                hidden = tf.nn.relu(tf.matmul(data, weights) + biases)\n",
    "                data = hidden\n",
    "                pre_units = next_units\n",
    "        # Linear\n",
    "        with g.name_scope('softmax_linear'):\n",
    "            weights = tf.Variable(\n",
    "                tf.truncated_normal([next_units, num_class],\n",
    "                                    stddev=1.0 / np.sqrt(float(next_units))),\n",
    "                name='weights')\n",
    "            biases = tf.Variable(tf.zeros([num_class]),\n",
    "                                 name='biases')\n",
    "            logits = tf.matmul(data, weights) + biases\n",
    "        return tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with graph_fully.as_default() as g:\n",
    "    layers = [128, 64]#neurons for each layer\n",
    "    y = multiHiddenLayer(x, layers)\n",
    "    #定义目标函数，采用交叉熵\n",
    "    cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "    #定义训练方式，梯度下降法\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "    #定义模型评价指标精确度\n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy： 0.1991\n",
      "Test Accuracy： 0.9385\n",
      "Test Accuracy： 0.9543\n",
      "Test Accuracy： 0.9627\n",
      "Test Accuracy： 0.9682\n",
      "Test Accuracy： 0.9651\n",
      "Test Accuracy： 0.9672\n",
      "Test Accuracy： 0.9651\n",
      "Test Accuracy： 0.9709\n",
      "Test Accuracy： 0.9687\n",
      "Test Accuracy： 0.9654\n"
     ]
    }
   ],
   "source": [
    "num_steps = 5001\n",
    "batch_size = 64\n",
    "#Create a session\n",
    "with tf.Session(graph=graph_fully) as sess:\n",
    "    #Initialize all the variables\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (mnist.train.labels.shape[0] - batch_size)\n",
    "        #Feed the placeholder    \n",
    "        batch_data = mnist.train.images[offset:(offset + batch_size), :]\n",
    "        batch_labels = mnist.train.labels[offset:(offset + batch_size), :]\n",
    "        #Pass the data\n",
    "        feed_dict = {x : batch_data, y_ : batch_labels}\n",
    "        _, loss = sess.run([train_step, cross_entropy], feed_dict=feed_dict)\n",
    "        if step%500 == 0:\n",
    "            result = sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})\n",
    "            print(\"Test Accuracy：\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network\n",
    "\n",
    "Following example shows how to create a convolutional neural network. In addition, 'name_scope' is taken into consideration for each layer. Reference:http://wiki.jikexueyuan.com/project/tensorflow-zh/how_tos/variable_scope.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create weights\n",
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial, name='weights')\n",
    "#Create biases\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial, name='biases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Convolutional function\n",
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "#Max pool function\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_relu_pool(input, kernel_shape, bias_shape):\n",
    "    # Create variable named \"weights\".\n",
    "    weights = weight_variable(kernel_shape)\n",
    "    # Create variable named \"biases\".\n",
    "    biases = bias_variable(bias_shape)\n",
    "    conv = conv2d(input, weights)\n",
    "    relu = tf.nn.relu(conv + biases)\n",
    "    pool = max_pool_2x2(relu)\n",
    "    return pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnnLayer(x, keep_prob):\n",
    "    x_image = tf.reshape(x, [-1,28,28,1])\n",
    "    #First Conv\n",
    "    with tf.variable_scope('hidden1'):\n",
    "        kernel_shape, bias_shape = [5, 5, 1, 32], [32] \n",
    "        h_pool1 = conv_relu_pool(x_image, kernel_shape, bias_shape)\n",
    "        \n",
    "    #Second Conv\n",
    "    with tf.variable_scope('hidden2'):\n",
    "        kernel_shape, bias_shape = [5, 5, 32, 64], [64] \n",
    "        h_pool2 = conv_relu_pool(h_pool1, kernel_shape, bias_shape)\n",
    "    \n",
    "    #Fully Connected Layer\n",
    "    with tf.variable_scope('fully_connected'):\n",
    "        W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "        b_fc1 = bias_variable([1024])\n",
    "        h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "        #Dropout, to prevent against overfitting      \n",
    "        h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "    #Softmax Layer\n",
    "    with tf.variable_scope('softmax_layer'):\n",
    "        W_fc2 = weight_variable([1024, 10])\n",
    "        b_fc2 = bias_variable([10])\n",
    "        logits = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "    return tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph_cnn = tf.Graph()\n",
    "with graph_cnn.as_default() as g:\n",
    "    #Create input placeholders\n",
    "    x = tf.placeholder(tf.float32, [None, 784])\n",
    "    y_ = tf.placeholder(tf.float32, [None,10])\n",
    "    #Define ropout probability placholder\n",
    "    keep_prob = tf.placeholder(\"float\")   \n",
    "\n",
    "    y = cnnLayer(x, keep_prob)\n",
    "    #Define cross-entropy as loss function\n",
    "    cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "    #Adam Optimizer\n",
    "    train_step = tf.train.AdamOptimizer(0.0005).minimize(cross_entropy)\n",
    "    #Define accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy： 0.1185\n",
      "Testing Accuracy： 0.972\n",
      "Testing Accuracy： 0.9818\n",
      "Testing Accuracy： 0.9868\n",
      "Testing Accuracy： 0.9883\n",
      "Testing Accuracy： 0.9878\n",
      "Testing Accuracy： 0.9886\n",
      "Testing Accuracy： 0.9901\n",
      "Testing Accuracy： 0.9909\n",
      "Testing Accuracy： 0.9903\n",
      "Testing Accuracy： 0.9894\n"
     ]
    }
   ],
   "source": [
    "num_steps = 5001\n",
    "batch_size = 64\n",
    "#Create a session\n",
    "with tf.Session(graph=graph_cnn) as sess:\n",
    "    #Initialize variables\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (mnist.train.labels.shape[0] - batch_size)\n",
    "        #Feed placeholders      \n",
    "        batch_data = mnist.train.images[offset:(offset + batch_size), :]\n",
    "        batch_labels = mnist.train.labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {x : batch_data, y_ : batch_labels, keep_prob: 0.5}\n",
    "        #Train\n",
    "        _, loss = sess.run([train_step, cross_entropy], feed_dict=feed_dict)\n",
    "        if step%500 == 0:\n",
    "            feed_dict = {x : mnist.test.images, y_ : mnist.test.labels, keep_prob: 1}\n",
    "            result = sess.run(accuracy, feed_dict=feed_dict)\n",
    "            print(\"Testing Accuracy：\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib import rnn\n",
    "# Network Parameters\n",
    "n_input = 28 # MNIST data input (img shape: 28*28)\n",
    "n_steps = 28 # timesteps\n",
    "n_hidden = 128 # hidden layer num of features\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph_rnn = tf.Graph()\n",
    "with graph_rnn.as_default():\n",
    "    # tf Graph input\n",
    "    x = tf.placeholder(tf.float32, [None, n_steps, n_input])\n",
    "    y_ = tf.placeholder(tf.float32, [None, n_classes])\n",
    "    def rnnLayer(x):\n",
    "        # 矩阵变换，\n",
    "        x = tf.transpose(x, [1, 0, 2])\n",
    "        # (n_steps*batch_size, n_input)\n",
    "        x = tf.reshape(x, [-1, n_input])\n",
    "        # 按照步长拆分 (batch_size, n_input)\n",
    "        x = tf.split(x, n_steps, 0)\n",
    "        with tf.variable_scope(\"rnn_lstm\"):\n",
    "            # 输出连接层权重\n",
    "            weights = tf.Variable(tf.random_normal([n_hidden, n_classes]), name='weights')\n",
    "            biases = tf.Variable(tf.random_normal([n_classes]), name='biases')\n",
    "            # Define a lstm cell with tensorflow\n",
    "            lstm_cell = rnn.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "            with tf.variable_scope('rnn_layer'):\n",
    "                # Get lstm cell output\n",
    "                outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "            # Linear activation, using rnn inner loop last output\n",
    "        logits = tf.matmul(outputs[-1], weights) + biases\n",
    "        return tf.nn.softmax(logits)\n",
    "\n",
    "    y = rnnLayer(x)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    cross_entropy = -tf.reduce_mean(y_* tf.log(y))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n",
    "\n",
    "    # Evaluate model\n",
    "    correct_pred = tf.equal(tf.argmax(y_,1), tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, Training Accuracy= 0.08594\n",
      "Iter 12800, Training Accuracy= 0.81250\n",
      "Iter 25600, Training Accuracy= 0.86719\n",
      "Iter 38400, Training Accuracy= 0.93750\n",
      "Iter 51200, Training Accuracy= 0.92969\n",
      "Iter 64000, Training Accuracy= 0.96094\n",
      "Iter 76800, Training Accuracy= 0.97656\n",
      "Iter 89600, Training Accuracy= 0.96875\n",
      "Iter 102400, Training Accuracy= 0.96875\n",
      "Iter 115200, Training Accuracy= 0.99219\n",
      "Iter 128000, Training Accuracy= 0.97656\n",
      "Iter 140800, Training Accuracy= 0.97656\n",
      "Iter 153600, Training Accuracy= 0.99219\n",
      "Iter 166400, Training Accuracy= 0.98438\n",
      "Iter 179200, Training Accuracy= 0.98438\n",
      "Iter 192000, Training Accuracy= 0.98438\n",
      "Iter 204800, Training Accuracy= 0.99219\n",
      "Iter 217600, Training Accuracy= 0.99219\n",
      "Iter 230400, Training Accuracy= 0.98438\n",
      "Iter 243200, Training Accuracy= 0.99219\n",
      "Iter 256000, Training Accuracy= 0.98438\n",
      "Iter 268800, Training Accuracy= 0.96875\n",
      "Iter 281600, Training Accuracy= 0.97656\n",
      "Iter 294400, Training Accuracy= 0.99219\n",
      "Iter 307200, Training Accuracy= 1.00000\n",
      "Iter 320000, Training Accuracy= 0.99219\n",
      "Iter 332800, Training Accuracy= 0.97656\n",
      "Iter 345600, Training Accuracy= 0.96875\n",
      "Iter 358400, Training Accuracy= 0.99219\n",
      "Iter 371200, Training Accuracy= 0.99219\n",
      "Iter 384000, Training Accuracy= 0.97656\n",
      "Iter 396800, Training Accuracy= 1.00000\n",
      "Iter 409600, Training Accuracy= 0.99219\n",
      "Iter 422400, Training Accuracy= 0.98438\n",
      "Iter 435200, Training Accuracy= 0.96875\n",
      "Iter 448000, Training Accuracy= 0.97656\n",
      "Iter 460800, Training Accuracy= 0.98438\n",
      "Iter 473600, Training Accuracy= 0.99219\n",
      "Iter 486400, Training Accuracy= 0.98438\n",
      "Iter 499200, Training Accuracy= 0.99219\n",
      "Iter 512000, Training Accuracy= 1.00000\n",
      "Iter 524800, Training Accuracy= 1.00000\n",
      "Iter 537600, Training Accuracy= 1.00000\n",
      "Iter 550400, Training Accuracy= 0.99219\n",
      "Iter 563200, Training Accuracy= 0.99219\n",
      "Iter 576000, Training Accuracy= 1.00000\n",
      "Iter 588800, Training Accuracy= 1.00000\n",
      "Iter 601600, Training Accuracy= 1.00000\n",
      "Iter 614400, Training Accuracy= 1.00000\n",
      "Iter 627200, Training Accuracy= 1.00000\n",
      "Iter 640000, Training Accuracy= 1.00000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.9848\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "num_steps = 5001\n",
    "batch_size = 128\n",
    "display_step = 100\n",
    "# Launch the graph\n",
    "with tf.Session(graph=graph_rnn) as sess:\n",
    "    # Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    # Keep training until reach max iterations\n",
    "    for step in np.arange(num_steps):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Reshape data to get 28 seq of 28 elements\n",
    "        batch_x = batch_x.reshape((batch_size, n_steps, n_input))\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y_: batch_y})\n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch accuracy\n",
    "            acc = sess.run(accuracy, feed_dict={x: batch_x, y_: batch_y})\n",
    "\n",
    "            print(\"Iter \" + str(step*batch_size) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for 128 mnist test images\n",
    "    test_data = mnist.test.images.reshape((-1, n_steps, n_input))\n",
    "    test_label = mnist.test.labels\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: test_data, y_: test_label}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-layer Rnn model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we create a two-layer LSTM neural network with dropout method to prevent against overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "layer_num = 2 #LSTM layer number\n",
    "graph_multirnn = tf.Graph()\n",
    "with graph_multirnn.as_default():\n",
    "    # tf Graph input\n",
    "    x = tf.placeholder(tf.float32, [None, n_steps, n_input])\n",
    "    y_ = tf.placeholder(tf.float32, [None, n_classes])\n",
    "    keep_prob =tf.placeholder(tf.float32)\n",
    "    def rnnLayer(x):\n",
    "        # 矩阵变换，\n",
    "        x = tf.transpose(x, [1, 0, 2])\n",
    "        # (n_steps*batch_size, n_input)\n",
    "        x = tf.reshape(x, [-1, n_input])\n",
    "        # 按照步长拆分 (batch_size, n_input)\n",
    "        x = tf.split(x, n_steps, 0)\n",
    "        with tf.variable_scope(\"rnn_lstm\"):\n",
    "            # 输出连接层权重\n",
    "            weights = tf.Variable(tf.random_normal([n_hidden, n_classes]), name='weights')\n",
    "            biases = tf.Variable(tf.random_normal([n_classes]), name='biases')\n",
    "            # Define a lstm cell with tensorflow\n",
    "            lstm_cell = rnn.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "            #Add dropout to prevent against overfitting\n",
    "            lstm_cell = rnn.DropoutWrapper(cell=lstm_cell, input_keep_prob=1.0, output_keep_prob=keep_prob)\n",
    "            #Create two layer LSTM network\n",
    "            mlstm_cell = rnn.MultiRNNCell([lstm_cell] * layer_num, state_is_tuple=True)\n",
    "            with tf.variable_scope('rnn_layer'):\n",
    "                # Get lstm cell output, using dynamic rnn, the input is one tensor\n",
    "                #outputs, states = tf.nn.dynamic_rnn(mlstm_cell, x, dtype=tf.float32)\n",
    "                #if we use static rnn, we need to unrollthe graphs and the tensor slices\n",
    "                #x must be a list of tensors\n",
    "                outputs, states = rnn.static_rnn(mlstm_cell, x, dtype=tf.float32)\n",
    "            # Linear activation, using rnn inner loop last output\n",
    "        logits = tf.matmul(outputs[-1], weights) + biases\n",
    "        return tf.nn.softmax(logits)\n",
    "\n",
    "    y = rnnLayer(x)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    cross_entropy = -tf.reduce_mean(y_* tf.log(y))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n",
    "\n",
    "    # Evaluate model\n",
    "    correct_pred = tf.equal(tf.argmax(y_,1), tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, Training Accuracy= 0.10938\n",
      "Iter 25600, Training Accuracy= 0.95312\n",
      "Iter 51200, Training Accuracy= 0.98438\n",
      "Iter 76800, Training Accuracy= 0.98438\n",
      "Iter 102400, Training Accuracy= 0.94531\n",
      "Iter 128000, Training Accuracy= 0.98438\n",
      "Iter 153600, Training Accuracy= 0.97656\n",
      "Iter 179200, Training Accuracy= 0.99219\n",
      "Iter 204800, Training Accuracy= 0.98438\n",
      "Iter 230400, Training Accuracy= 0.97656\n",
      "Iter 256000, Training Accuracy= 0.98438\n",
      "Iter 281600, Training Accuracy= 1.00000\n",
      "Iter 307200, Training Accuracy= 0.99219\n",
      "Iter 332800, Training Accuracy= 0.99219\n",
      "Iter 358400, Training Accuracy= 0.97656\n",
      "Iter 384000, Training Accuracy= 0.99219\n",
      "Iter 409600, Training Accuracy= 1.00000\n",
      "Iter 435200, Training Accuracy= 0.99219\n",
      "Iter 460800, Training Accuracy= 0.99219\n",
      "Iter 486400, Training Accuracy= 0.98438\n",
      "Iter 512000, Training Accuracy= 0.99219\n",
      "Iter 537600, Training Accuracy= 1.00000\n",
      "Iter 563200, Training Accuracy= 1.00000\n",
      "Iter 588800, Training Accuracy= 0.99219\n",
      "Iter 614400, Training Accuracy= 1.00000\n",
      "Iter 640000, Training Accuracy= 1.00000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.9887\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "num_steps = 5001\n",
    "batch_size = 128\n",
    "display_step = 200\n",
    "# Launch the graph\n",
    "with tf.Session(graph=graph_multirnn) as sess:\n",
    "    # Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    # Keep training until reach max iterations\n",
    "    for step in np.arange(num_steps):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Reshape data to get 28 seq of 28 elements\n",
    "        batch_x = batch_x.reshape((batch_size, n_steps, n_input))\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y_: batch_y, keep_prob: 0.5})\n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch accuracy\n",
    "            acc = sess.run(accuracy, feed_dict={x: batch_x, y_: batch_y, keep_prob:1.0})\n",
    "\n",
    "            print(\"Iter \" + str(step*batch_size) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for 128 mnist test images\n",
    "    test_data = mnist.test.images.reshape((-1, n_steps, n_input))\n",
    "    test_label = mnist.test.labels\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: test_data, y_: test_label, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Bi-directional Rnn model\n",
    "\n",
    "This example shows how to apply Bidirectional RNN model to MNIST images, actually it leverages two RNN neetwork, one forward, one backward. Reference: https://github.com/aymericdamien/TensorFlow-Examples/tree/master/notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "num_steps = 10000\n",
    "batch_size = 128\n",
    "display_step = 500\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 28 # MNIST data input (img shape: 28*28)\n",
    "n_steps = 28 # timesteps\n",
    "n_hidden = 128 # hidden layer num of features\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph_biRnn = tf.Graph()\n",
    "with graph_biRnn.as_default():\n",
    "    # placeholder input\n",
    "    x = tf.placeholder(tf.float32, [None, n_steps, n_input])\n",
    "    y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "\n",
    "    def birnnLayer(x):\n",
    "\n",
    "        # Prepare data shape to match `bidirectional_rnn` function requirements\n",
    "        # Current data input shape: (batch_size, n_steps, n_input)\n",
    "        # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)\n",
    "    \n",
    "        # Permuting batch_size and n_steps\n",
    "        x = tf.transpose(x, [1, 0, 2])\n",
    "        # Reshape to (n_steps*batch_size, n_input)\n",
    "        x = tf.reshape(x, [-1, n_input])\n",
    "        # Split to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "        x = tf.split(x, n_steps, 0)\n",
    "        \n",
    "        # Define weights\n",
    "        weights = {\n",
    "              # Hidden layer weights => 2*n_hidden because of foward + backward cells\n",
    "              'out': tf.Variable(tf.random_normal([2*n_hidden, n_classes]), name='weights')}\n",
    "        biases = {\n",
    "              'out': tf.Variable(tf.random_normal([n_classes]), name='biases')}\n",
    "\n",
    "        # Define lstm cells with tensorflow\n",
    "        # Forward direction cell\n",
    "        with tf.variable_scope('BiRnnLayer'):\n",
    "            lstm_fw_cell = rnn.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "            # Backward direction cell\n",
    "            lstm_bw_cell = rnn.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "            #   Get lstm cell output\n",
    "            with tf.variable_scope('BiRnn_Structure'):\n",
    "                try:\n",
    "                    outputs, _, _ = rnn.static_bidirectional_rnn(lstm_fw_cell, \n",
    "                                                                 lstm_bw_cell, x,\n",
    "                                                                 dtype=tf.float32)\n",
    "                except Exception: # Old TensorFlow version only returns outputs not states\n",
    "                    outputs = rnn.static_bidirectional_rnn(lstm_fw_cell, \n",
    "                                                           lstm_bw_cell, x,\n",
    "                                                           dtype=tf.float32)\n",
    "\n",
    "        # Linear activation, using rnn inner loop last output\n",
    "        return tf.add(tf.matmul(outputs[-1], weights['out']), biases['out'])\n",
    "\n",
    "    pred = birnnLayer(x)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n",
    "\n",
    "    # Evaluate model\n",
    "    correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, Minibatch Loss= 2.148260, Training Accuracy= 0.31250\n",
      "Iter 64000, Minibatch Loss= 0.228522, Training Accuracy= 0.92969\n",
      "Iter 128000, Minibatch Loss= 0.102823, Training Accuracy= 0.96875\n",
      "Iter 192000, Minibatch Loss= 0.168021, Training Accuracy= 0.96094\n",
      "Iter 256000, Minibatch Loss= 0.085315, Training Accuracy= 0.97656\n",
      "Iter 320000, Minibatch Loss= 0.040498, Training Accuracy= 0.98438\n",
      "Iter 384000, Minibatch Loss= 0.009238, Training Accuracy= 1.00000\n",
      "Iter 448000, Minibatch Loss= 0.012013, Training Accuracy= 1.00000\n",
      "Iter 512000, Minibatch Loss= 0.016879, Training Accuracy= 0.99219\n",
      "Iter 576000, Minibatch Loss= 0.006823, Training Accuracy= 1.00000\n",
      "Iter 640000, Minibatch Loss= 0.001968, Training Accuracy= 1.00000\n",
      "Iter 704000, Minibatch Loss= 0.014346, Training Accuracy= 0.99219\n",
      "Iter 768000, Minibatch Loss= 0.007600, Training Accuracy= 1.00000\n",
      "Iter 832000, Minibatch Loss= 0.001743, Training Accuracy= 1.00000\n",
      "Iter 896000, Minibatch Loss= 0.004569, Training Accuracy= 1.00000\n",
      "Iter 960000, Minibatch Loss= 0.003858, Training Accuracy= 1.00000\n",
      "Iter 1024000, Minibatch Loss= 0.003035, Training Accuracy= 1.00000\n",
      "Iter 1088000, Minibatch Loss= 0.003761, Training Accuracy= 1.00000\n",
      "Iter 1152000, Minibatch Loss= 0.002384, Training Accuracy= 1.00000\n",
      "Iter 1216000, Minibatch Loss= 0.000756, Training Accuracy= 1.00000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.992188\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session(graph=graph_biRnn) as sess:\n",
    "    # Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    # Keep training until reach max iterations\n",
    "    for step in np.arange(num_steps):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Reshape data to get 28 seq of 28 elements\n",
    "        batch_x = batch_x.reshape((batch_size, n_steps, n_input))\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch accuracy\n",
    "            acc = sess.run(accuracy, feed_dict={x: batch_x, y: batch_y})\n",
    "            # Calculate batch loss\n",
    "            loss = sess.run(cross_entropy, feed_dict={x: batch_x, y: batch_y})\n",
    "            print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for 128 mnist test images\n",
    "    test_data = mnist.test.images.reshape((-1, n_steps, n_input))\n",
    "    test_label = mnist.test.labels\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: test_data, y: test_label}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Adversarial Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows how to use generative adversarial model to create fake MNIST images and try to distinguish them. Referrence: https://github.com/wiseodd/generative-models/tree/master/GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import required packages\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, define those parameters for discriminator and generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define xavier initializing method, tries to keep variance concentrated\n",
    "#http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2010_GlorotB10.pdf\n",
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "    return tf.random_normal(shape=size, stddev=xavier_stddev)\n",
    "\n",
    "#Parameters for discriminator\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "\n",
    "D_W1 = tf.Variable(xavier_init([784, 128]))\n",
    "D_b1 = tf.Variable(tf.zeros(shape=[128]))\n",
    "\n",
    "D_W2 = tf.Variable(xavier_init([128, 1]))\n",
    "D_b2 = tf.Variable(tf.zeros(shape=[1]))\n",
    "\n",
    "theta_D = [D_W1, D_W2, D_b1, D_b2]\n",
    "\n",
    "#Parameters for generator\n",
    "Z = tf.placeholder(tf.float32, shape=[None, 100])\n",
    "\n",
    "G_W1 = tf.Variable(xavier_init([100, 128]))\n",
    "G_b1 = tf.Variable(tf.zeros(shape=[128]))\n",
    "\n",
    "G_W2 = tf.Variable(xavier_init([128, 784]))\n",
    "G_b2 = tf.Variable(tf.zeros(shape=[784]))\n",
    "\n",
    "theta_G = [G_W1, G_W2, G_b1, G_b2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then define the architecture for generator and discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_Z(m, n):\n",
    "    return np.random.uniform(-1., 1., size=[m, n])\n",
    "\n",
    "#Generate a digit from random noise\n",
    "#Return a fake digit \n",
    "def generator(z):\n",
    "    G_h1 = tf.nn.relu(tf.matmul(z, G_W1) + G_b1)\n",
    "    G_log_prob = tf.matmul(G_h1, G_W2) + G_b2\n",
    "    G_prob = tf.nn.sigmoid(G_log_prob)\n",
    "\n",
    "    return G_prob\n",
    "\n",
    "#Discriminate the input digit\n",
    "#Return a probability\n",
    "def discriminator(x):\n",
    "    D_h1 = tf.nn.relu(tf.matmul(x, D_W1) + D_b1)\n",
    "    D_logit = tf.matmul(D_h1, D_W2) + D_b2\n",
    "    D_prob = tf.nn.sigmoid(D_logit)\n",
    "\n",
    "    return D_prob, D_logit\n",
    "\n",
    "#Plot the samples\n",
    "def plot(samples):\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "G_sample = generator(Z)\n",
    "#For real digit\n",
    "D_real, D_logit_real = discriminator(X)\n",
    "#For generated digit\n",
    "D_fake, D_logit_fake = discriminator(G_sample)\n",
    "\n",
    "# D_loss = -tf.reduce_mean(tf.log(D_real) + tf.log(1. - D_fake))\n",
    "# G_loss = -tf.reduce_mean(tf.log(D_fake))\n",
    "\n",
    "# Alternative losses:\n",
    "# -------------------\n",
    "D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_real, \n",
    "                                                                     labels=tf.ones_like(D_logit_real)))\n",
    "D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, \n",
    "                                                                     labels=tf.zeros_like(D_logit_fake)))\n",
    "D_loss = D_loss_real + D_loss_fake\n",
    "G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, \n",
    "                                                                labels=tf.ones_like(D_logit_fake)))\n",
    "\n",
    "D_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list=theta_D)\n",
    "G_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list=theta_G)\n",
    "\n",
    "mb_size = 128\n",
    "Z_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0\n",
      "D loss: 0.9787\n",
      "G_loss: 2.71\n",
      "\n",
      "Iter: 1000\n",
      "D loss: 0.003484\n",
      "G_loss: 11.91\n",
      "\n",
      "Iter: 2000\n",
      "D loss: 0.005783\n",
      "G_loss: 6.416\n",
      "\n",
      "Iter: 3000\n",
      "D loss: 0.065\n",
      "G_loss: 6.45\n",
      "\n",
      "Iter: 4000\n",
      "D loss: 0.0915\n",
      "G_loss: 6.034\n",
      "\n",
      "Iter: 5000\n",
      "D loss: 0.1342\n",
      "G_loss: 5.43\n",
      "\n",
      "Iter: 6000\n",
      "D loss: 0.3739\n",
      "G_loss: 4.435\n",
      "\n",
      "Iter: 7000\n",
      "D loss: 0.6105\n",
      "G_loss: 4.666\n",
      "\n",
      "Iter: 8000\n",
      "D loss: 0.3533\n",
      "G_loss: 3.829\n",
      "\n",
      "Iter: 9000\n",
      "D loss: 0.4286\n",
      "G_loss: 3.31\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "if not os.path.exists('out/'):\n",
    "    os.makedirs('out/')\n",
    "\n",
    "i = 0\n",
    "\n",
    "for it in np.arange(10000):\n",
    "    if it % 1000 == 0:\n",
    "        samples = sess.run(G_sample, feed_dict={Z: sample_Z(16, Z_dim)})\n",
    "\n",
    "        fig = plot(samples)\n",
    "        plt.savefig('out/{}.png'.format(str(i).zfill(3)), bbox_inches='tight')\n",
    "        i += 1\n",
    "        plt.close(fig)\n",
    "\n",
    "    X_mb, _ = mnist.train.next_batch(mb_size)\n",
    "\n",
    "    _, D_loss_curr = sess.run([D_solver, D_loss], feed_dict={X: X_mb, Z: sample_Z(mb_size, Z_dim)})\n",
    "    _, G_loss_curr = sess.run([G_solver, G_loss], feed_dict={Z: sample_Z(mb_size, Z_dim)})\n",
    "\n",
    "    if it % 1000 == 0:\n",
    "        print('Iter: {}'.format(it))\n",
    "        print('D loss: {:.4}'. format(D_loss_curr))\n",
    "        print('G_loss: {:.4}'.format(G_loss_curr))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPMAAADuCAYAAADsvjF6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAxR3vVPTvYeFAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x145bc9c18>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
